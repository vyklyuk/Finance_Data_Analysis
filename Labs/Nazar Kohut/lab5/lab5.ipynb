{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "ab2842ac-cbcb-46fc-8dc6-1c8829ebbd07",
      "metadata": {},
      "outputs": [],
      "source": [
        "<center>\n    <img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMSkillsNetwork-GPXX0X57EN/SN_web_lightmode.png?1679073235843\" width=\"300\" alt=\"cognitiveclass.ai logo\">\n</center>\n\n# Investigation of BTC/BUSD cryptocurrency using ADOSC, NATR, TRANGE indicators, and other cryptocurrencies.\n\n\n# Lab5 (Model Evaluation and Refinement)\n\nEstimated time needed: **30** minutes\n\n## Objectives\n\nAfter completing this lab you will be able to:\n\n*   Evaluate and refine prediction models\n\n<h3>Table of Contents</h3>\n\n<div class=\"alert alert-block alert-info\" style=\"margin-top: 20px\">\n<ol>\n    <li><u>Training and Testing</u></li>\n    <li><u>Cross-Validation</u></li>\n    <li><u>Overfitting, Underfitting and Model Selection</u></li>\n    <li><u>Ridge Regression</u></li>\n    <li><u>Grid Search</u></li>\n</ol>\n\n</div>\n\n<hr>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6eec3e16-0c8a-4460-91eb-18155a13d281",
      "metadata": {},
      "outputs": [],
      "source": [
        "#### ***Dataset description***\n\nThe dataset used in this lab contains time-series data on various attributes related to Bitcoin (BTC) and other cryptocurrencies, aggregated at 1-minute intervals. The dataset index represents the time period for which the data is reported(1 minute). Also the dataset contains binned average prices of other cryptocurrencies. \n\n<hr>\n\n**Attributes:**\n\n* ***General:***\n    * `open` - the opening price of a **BTC** during a specific time period.\n    * `high` - the highest price of a **BTC** during a specific time period.\n    * `low` - the lowest price of a **BTC** during a specific time period.\n    * `close` - the closing price of a **BTC** during a specific time period.\n    * `rec_count` - the number of records or data points in the dataset for a given time period.\n    * `volume` - the total amount of trading activity (buying and selling) for a **BTC** during a specific time period.\n    * `avg_price` - the average price of a **BTC** during a specific time period.\n\n\n* ***Indicators***\n    * `ADOSC` - an indicator used in technical analysis to measure the momentum of buying and selling pressure for ***Bitcoin***.\n    * `NATR` - an indicator used in technical analysis to measure the volatility of ***Bitcoin***.\n    * `TRANGE` - an indicator used in technical analysis to measure the range of prices (from high to low) for ***Bitcoin*** during a specific time period.\n\n\n* ***Other cryptocurrencies:***\n    * `ape_avg_price` - the average price of ***APE*** during a specific time period.\n    * `bnb_avg_price` - the average price of ***BNB*** during a specific time period.\n    * `doge_avg_price` - the average price of ***DOGE coin*** during a specific time period.\n    * `eth_avg_price` - the average price of ***Ethereum*** during a specific time period.\n    * `xrp_avg_price` - the average price of ***XRP*** during a specific time period.\n    * `matic_avg_price` - the average price of ***MATIC*** during a specific time period.\n* ***Categorical:***\n    * `category` - binned ***average price for BTC(`avg_price`)*** to bins: `high`, `medium-high`, `medium`, `medium-low`, `low`.\n    * `ape_category` - binned `ape_avg_price` to bins: `high`, `medium-high`, `medium`, `medium-low`, `low`.\n    * `bnb_category` - binned `bnb_avg_price` to bins: `high`, `medium-high`, `medium`, `medium-low`, `low`.\n    * `doge_category` - binned `doge_avg_price` to bins: `high`, `medium-high`, `medium`, `medium-low`, `low`.\n    * `eth_category` - binned `eth_avg_price` to bins: `high`, `medium-high`, `medium`, `medium-low`, `low`.\n    * `xrp_category` - binned `xrp_avg_price` to bins: `high`, `medium-high`, `medium`, `medium-low`, `low`.\n    * `matic_category` - binned `matic_avg_price` to bins: `high`, `medium-high`, `medium`, `medium-low`, `low`.\n    \n<hr>\n\n*The indicators `ADOSC`, `NATR`, and `TRANGE` are used in technical analysis to provide insights into the momentum, volatility, and price ranges of financial instruments or assets. The other attributes represent the average prices of different cryptocurrencies during a specific time period.*\n\n<hr>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "45cbf218-ca6b-48ea-ba0d-23965310d0c2",
      "metadata": {},
      "outputs": [],
      "source": [
        "In previous lab we trained ML models and retrieved some metrics for our models on test data. In this one, we will explain this metrics more in depth."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d3ec5d80-c69d-46c5-86d4-019ba1ca1aa1",
      "metadata": {},
      "outputs": [],
      "source": [
        "#### Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "id": "6a2a28a2-662d-45c1-950c-cb5bdf976129",
      "metadata": {},
      "outputs": [],
      "source": [
        "!mamba install pandas -y\n!mamba install numpy -y\n!mamba install matplotlib -y\n!mamba install scipy -y\n!mamba install seaborn -y \n!mamba install statsmodels -y\n!mamba install scikit-learn -y \n!mamba install tqdm -y"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8e8eb813-140a-4dd9-8a61-6d57fd072f9f",
      "metadata": {},
      "outputs": [],
      "source": [
        "Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "id": "619be568-7229-4a28-8221-0a7be0e70563",
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom tqdm import tqdm\nfrom ipywidgets import interact, interactive, fixed, interact_manual\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import TimeSeriesSplit\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.linear_model import Ridge\nfrom sklearn.model_selection import GridSearchCV\n\n%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "89e4b9f5-c7a3-4fcf-801d-3d806c57ebbe",
      "metadata": {},
      "outputs": [],
      "source": [
        "Intialize path to our dataset(the one we saved in previous lab). "
      ]
    },
    {
      "cell_type": "code",
      "id": "ee315a16-e190-4802-8766-d2546fa81ff6",
      "metadata": {},
      "outputs": [],
      "source": [
        "path = 'https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMSkillsNetwork-GPXX0X57EN/BTCBUSD_1min_categories.csv'"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e163407d-8213-4ccd-8dec-4276c527e130",
      "metadata": {},
      "outputs": [],
      "source": [
        "Read dataset from path."
      ]
    },
    {
      "cell_type": "code",
      "id": "a7d188f3-7910-4ed8-912c-56bd42375edc",
      "metadata": {},
      "outputs": [],
      "source": [
        "df = pd.read_csv(path)\n\n# converting index to datetime\ndf.index = pd.to_datetime(df.index)\ndf.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c0b15cbf-bceb-4e3b-b705-986c47a76f9c",
      "metadata": {},
      "outputs": [],
      "source": [
        "Now, let's retrieve columns and shape of our dataFrame"
      ]
    },
    {
      "cell_type": "code",
      "id": "9ebf58ca-4d73-4960-95cc-78248d8540f9",
      "metadata": {},
      "outputs": [],
      "source": [
        "print(df.columns, df.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e7352613-a46d-4946-a269-c5cc82d58e0b",
      "metadata": {},
      "outputs": [],
      "source": [
        "##### **Functions for Plotting**"
      ]
    },
    {
      "cell_type": "code",
      "id": "5e69ec54-c6c6-4b4e-bee4-c8ddc41eb214",
      "metadata": {},
      "outputs": [],
      "source": [
        "def DistributionPlot(RedFunction, BlueFunction, RedName, BlueName, Title, figsize=(12, 10)):\n    width, height = figsize\n    plt.figure(figsize=(width, height))\n    \n    ax1 = sns.kdeplot(RedFunction, color=\"r\", label=RedName)\n    ax2 = sns.kdeplot(BlueFunction, color=\"b\", label=BlueName, ax=ax1)\n\n    plt.title(Title)\n    plt.xlabel('Price (in dollars)')\n    plt.ylabel('Proportion')\n    plt.legend()\n\n    plt.show()\n    plt.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5714fc40-acfa-4e30-8b01-f8f4d5b3982c",
      "metadata": {},
      "outputs": [],
      "source": [
        "## Training and Testing\n\n<p>Just like in previous lab let's split our data into train and test samples:</p>\n"
      ]
    },
    {
      "cell_type": "code",
      "id": "ec2730cb-b6eb-4b66-9f3a-2464cfb7fc6f",
      "metadata": {},
      "outputs": [],
      "source": [
        "# y is a target value column - the value we want to predict\ny = df['avg_price']\n# deleting target column from X dataset\nX = df.drop('avg_price', axis=1)\n\n# setting test_size to 0.2 so our test set will be of size 20% \n# using shuffle=False to split data into 2 windows(80% of train data that happened before other 20% of train)\n# or in other words we predict 20 percent of prices that will happen in future\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3eb2d629-4839-4aff-9813-f69f5b90eec6",
      "metadata": {},
      "outputs": [],
      "source": [
        "Let's look at the shape of training and test data"
      ]
    },
    {
      "cell_type": "code",
      "id": "fae44a1e-0e41-42c8-9559-b057da226583",
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Number of training samples:\",X_train.shape[0])\nprint(\"Number of test samples :\", X_test.shape[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2b7daada-82d0-4bdf-b65c-54252620dd03",
      "metadata": {},
      "outputs": [],
      "source": [
        "The `test_size` parameter sets the proportion of data that is split into the testing set. In the above, the testing set is 20% of the total dataset.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a27cbc47-62a5-41a8-98f7-8112b6023bc4",
      "metadata": {},
      "outputs": [],
      "source": [
        "Just like in previous lab, let's train Linear regression model"
      ]
    },
    {
      "cell_type": "code",
      "id": "1a1d446f-5315-4bfd-966f-eb038aa1ba2d",
      "metadata": {},
      "outputs": [],
      "source": [
        "lr = LinearRegression()\nlr.fit(X_train[['doge_avg_price']], y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a4bb0ac5-add2-4f47-91e1-0dd9f01f68d0",
      "metadata": {},
      "outputs": [],
      "source": [
        "Let's calculate $R^2$ on train and test data(in previous lab we calculated only test metrics, but it is a good practice to calculate both, in order to avoid underfitting and overfitting - you will learn more about these terms later in this lab)"
      ]
    },
    {
      "cell_type": "code",
      "id": "ab5beff0-11ab-48cb-ab08-5448d7cf5218",
      "metadata": {},
      "outputs": [],
      "source": [
        "print(f\"Test r2 score: {lr.score(X_test[['doge_avg_price']], y_test):.4f}\")\nprint(f\"Train r2 score: {lr.score(X_train[['doge_avg_price']], y_train):.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "94d93596-5218-4d8d-b000-f3d3c9b79caa",
      "metadata": {},
      "outputs": [],
      "source": [
        "We can see that test and train $R^2$ score are relatively high(as for Linear regression model) and have almost the same $R^2$ score."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "06d082c0-f64e-44b3-b54f-9a63f51c39ee",
      "metadata": {},
      "outputs": [],
      "source": [
        "<div class=\"alert alert-danger alertdanger\" style=\"margin-top: 20px\">\n  <b style=\"font-size: 2em; font-weight: bold;\"> Question  #1: </b>\n\n  <b>Use the function \"train_test_split\" to split up the dataset such that 10% of the data samples will be utilized for testing. Set the parameter `shuffle` equal to `False`. The output of the function should be the following:  `X_train1` , `X_test1`, `y_train1` and  `y_test1`.</b>\n    \n</div>\n"
      ]
    },
    {
      "cell_type": "code",
      "id": "9774c3ca-2901-4fbe-ab10-2b4dc3091018",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Write your code below and press Shift+Enter to execute "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "03c0e216-57d2-4572-9c13-60834a0edf11",
      "metadata": {},
      "outputs": [],
      "source": [
        "<details><summary>Click here for the solution</summary>\n\n```python\nX_train1, X_test1, y_train1, y_test1 = train_test_split(X, y, shuffle=False, test_size=0.1) \nprint(\"Number of training samples:\",X_train1.shape[0])\nprint(\"Number of test samples :\", X_test1.shape[0])\n```\n\n</details>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d5d2bfba-2af6-4a25-adcd-8ab9c34fa790",
      "metadata": {},
      "outputs": [],
      "source": [
        "<div class=\"alert alert-danger alertdanger\" style=\"margin-top: 20px\">\n  <b style=\"font-size: 2em; font-weight: bold;\"> Question  #2: </b>\n\n  <b>Using data you received in task № 1 train linear regression model(Use <code>doge_avg_price</code> as a predictor) and call it <code>lr1</code>..</b>\n    \n</div>\n"
      ]
    },
    {
      "cell_type": "code",
      "id": "edebe924-b9cb-4aff-8b70-653fa3409564",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Write your code below and press Shift+Enter to execute"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5b3af8ee-3bc8-4837-87e7-9efdda71a3ac",
      "metadata": {},
      "outputs": [],
      "source": [
        "<details><summary>Click here for the solution</summary>\n\n```python\n\nlr1=LinearRegression()\nlr1.fit(X_train1[['doge_avg_price']], y_train1)\n```\n\n</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cc47e606-d507-4fc9-a7c9-c02493e7255d",
      "metadata": {},
      "outputs": [],
      "source": [
        "<div class=\"alert alert-danger alertdanger\" style=\"margin-top: 20px\">\n  <b style=\"font-size: 2em; font-weight: bold;\"> Question  #3: </b>\n\n  <b>Find the $R^2$  on the test data using 10% of the dataset on test and train data; Make conclustions.</b>\n    \n</div>\n\n"
      ]
    },
    {
      "cell_type": "code",
      "id": "5315a400-1344-4e28-9126-4fceba8dcafb",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Write your code below and press Shift+Enter to execute"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e2eddd73-4590-46f6-b855-9b489a9a36e4",
      "metadata": {},
      "outputs": [],
      "source": [
        "<details><summary>Click here for the solution</summary>\n\n```python\nprint(f\"Test r2 score: {lr1.score(X_test1[['doge_avg_price']], y_test1)}\")\nprint(f\"Train r2 score: {lr1.score(X_train1[['doge_avg_price']], y_train1)}\")\n```\nWe can see that test score is negative(it means that model performance is really poor) at the same time training data has r2 of 0.75, so it is clear sign of overfitting(model performs well on train, but not on test | it does not generalize well).\n</details>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5e243722-21b2-4f2f-86a3-1414fade72d7",
      "metadata": {},
      "outputs": [],
      "source": [
        "## Cross-Validation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cde3e476-cc9a-4074-90d4-54dc751941a8",
      "metadata": {},
      "outputs": [],
      "source": [
        "When working with time series data, like BTC and other cryptocurrencies, **it's essential to evaluate the performance of your model using cross-validation** in addition to evaluating it on a single test dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "590e429a-f75a-45bf-968b-8d6729deb365",
      "metadata": {},
      "outputs": [],
      "source": [
        "**Cross-validation is important because it helps to estimate how well your model can generalize to new, unseen data.** \n\n**Time series data** is **sequential**, and **future observations are often dependent on past observations**, so **the model needs to be able to capture this temporal relationship to predict future values accurately.**\n\nIn case of time series data, two common techniques for cross-validation are **forward chaining cross-validation** and **rolling window cross-validation**. **Forward chaining cross-validation involves training the model on earlier data and testing it on subsequent data**, while **rolling window cross-validation involves training the model on a fixed window of data and testing it on a subsequent window**."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8bdac5ce-0f2a-42a0-96b5-d753fdffd105",
      "metadata": {},
      "outputs": [],
      "source": [
        "We will use forward chaining cross-validation(explanation of why we use forward chaining cross-validation will be below):"
      ]
    },
    {
      "cell_type": "code",
      "id": "2de22d9e-d4d7-4c96-aefb-9731d1de9ff6",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Set the size of the validation set to be used for each iteration\nvalidation_size = 60 # we use 1 hour\n\n# Convert DataFrame to NumPy array\nX_array = X[['doge_avg_price']].to_numpy()\ny_array = y.to_numpy()\n\n# Create an empty list to store the mean squared error for each fold\nmse_scores = []\nr2_scores = []\n\n# Instantiate the model\nmodel = LinearRegression()\n\n# Loop over each data point in the sequence\nfor i in range(validation_size, X.shape[0]):\n    # Split the data into training and validation sets\n    X_train_i = X_array[i-validation_size:i, :]\n    y_train_i = y_array[i-validation_size:i]\n    X_valid_i = X_array[i:i+1, :]\n    y_valid_i = y_array[i:i+1]\n    \n    # Train the model on the training data\n    model.fit(X_train_i, y_train_i)\n    \n    # Use the model to predict the validation data\n    y_pred_i = model.predict(X_valid_i)\n    \n    # Calculate the mean squared error for this fold\n    mse = mean_squared_error(y_valid_i, y_pred_i)\n\n    # Add the mean squared error to the list of mse scores\n    mse_scores.append(mse)\n\n# Calculate the overall mean squared error across all folds\nmean_mse = np.mean(mse_scores)\n\n# Print the mean squared error\nprint(F\"Overall Mean squared error: {mean_mse:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f44c14f8-2334-4d1c-a606-0a6ae3d60ab3",
      "metadata": {},
      "outputs": [],
      "source": [
        "The **forward chaining cross-validation method is a way to simulate how well your model will perform in real-world situations where we are getting new data over time**, and **we want to train your model on that new data as it arrives**.\n\nIn this approach, **we train the model on a fixed size of historical data and evaluate it on the next data point in the sequence, which represents the new data that just arrived**. By doing this for every data point in the sequence, so we simulate the process of training and testing our model on new data as it arrives over time.\n\n***In other words:***\n\nThe **goal of this approach is to evaluate how well your model can generalize to new data over time**, which is important in real-world applications where your data is constantly changing."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ebf6ae63-a26a-4e97-9242-c136fce2afcc",
      "metadata": {},
      "outputs": [],
      "source": [
        "**Why we did not use `sklearn.model_selection.cross_val_score`?**\n\nBecause, `sklearn.model_selection.cross_val_score` does not perform forward chaining cross-validation by default. Instead it implements k-fold cross-validation, which shuffles the data and divides it into `k` equally sized folds. Each fold is used as a test set once while the remaining folds are used as the training set. The cross-validation process is repeated `k` times and the average score is calculated across all folds."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "76718f91-c4fc-4f4e-8786-c86296c0cf85",
      "metadata": {},
      "outputs": [],
      "source": [
        "Let's plot Mean squared error that we calculated over each fold:"
      ]
    },
    {
      "cell_type": "code",
      "id": "a088b3ea-17d6-4117-aa66-eff7563dfde7",
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_mse_scores(mse_scores):\n    \"\"\"\n    Plots a line graph of the mean squared error (MSE) scores.\n    \n    Args:\n        mse_scores (list): A list of mean squared error scores.\n    \"\"\"\n    # Create a line plot of the mean squared error scores\n    plt.plot(mse_scores)\n    \n    # Set the x-axis label\n    plt.xlabel('Folds')\n    \n    # Set the y-axis label\n    plt.ylabel('Mean Squared Error')\n    \n    # Set the title of the plot\n    plt.title('Mean Squared Error Scores')\n    \n    # Show the plot\n    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "id": "46589c53-82ec-4b9d-82b7-2952c223b7e2",
      "metadata": {},
      "outputs": [],
      "source": [
        "plot_mse_scores(mse_scores)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "42d26208-3235-44c2-b508-128894dda839",
      "metadata": {},
      "outputs": [],
      "source": [
        "From the plot we can see that our model would make mistakes frequently."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "78166072-b48b-47be-8a46-db3ccad7f979",
      "metadata": {},
      "outputs": [],
      "source": [
        "## Overfitting, Underfitting and Model Selection\n\nIn previous lab we usually calculated test metrics, but it is a good practice to calculate train metrics as well to get better insights. \n\n***Why?***\n\n**By comparing test metrics to train metrics, you can determine whether the model is overfitting or underfitting. If the test metrics are significantly worse than the train metrics, it's a sign that the model may be overfitting to the training data.** In other words, it helps in identifying potential issues and make improvements to the model to improve its overall performance and generalization to new data"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d0bf6a22-041d-4a32-825d-d6253cf4f9f5",
      "metadata": {},
      "outputs": [],
      "source": [
        "Let's create **Multiple Linear Regression objects and train the model just like in previous lab**(using same features as in previous lab).\n"
      ]
    },
    {
      "cell_type": "code",
      "id": "336d6a71-53be-47db-9907-9a198aa2230c",
      "metadata": {},
      "outputs": [],
      "source": [
        "mlr = LinearRegression()\nmlr.fit(X_train[['doge_avg_price', 'eth_avg_price', 'xrp_avg_price']], y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bbc4c18c-33f7-411d-8fda-420d949fbbcf",
      "metadata": {},
      "outputs": [],
      "source": [
        "Prediction using training data:\n"
      ]
    },
    {
      "cell_type": "code",
      "id": "4ea00e55-6da9-49ce-8d15-87bb4f5b3a03",
      "metadata": {},
      "outputs": [],
      "source": [
        "yhat_train = mlr.predict(X_train[['doge_avg_price', 'eth_avg_price', 'xrp_avg_price']])\nyhat_train[0:5]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a3efb426-882f-444d-ba63-211b238215d1",
      "metadata": {},
      "outputs": [],
      "source": [
        "Prediction using test data:\n"
      ]
    },
    {
      "cell_type": "code",
      "id": "39d5db49-cc20-4101-8283-72383339d29c",
      "metadata": {},
      "outputs": [],
      "source": [
        "yhat_test = mlr.predict(X_test[['doge_avg_price', 'eth_avg_price', 'xrp_avg_price']])\nyhat_test[0:5]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0d833fa1-8de4-4d3c-a1d9-15f08859d6de",
      "metadata": {},
      "outputs": [],
      "source": [
        "Let's examine the distribution of the predicted values of the training data.\n"
      ]
    },
    {
      "cell_type": "code",
      "id": "08295346-5089-474c-ba60-0ba140aae52c",
      "metadata": {},
      "outputs": [],
      "source": [
        "Title = 'Distribution  Plot of  Predicted Value Using Training Data vs Training Data Distribution'\nDistributionPlot(y_train, yhat_train, \"Actual Values (Train)\", \"Predicted Values (Train)\", Title)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "43c78c85-6265-447c-a458-9aaa430f9956",
      "metadata": {},
      "outputs": [],
      "source": [
        "**Figure 1:** Plot of predicted values using the training data compared to the actual values of the training data.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7e7b3f58-8de9-464b-97db-d221aaef05bc",
      "metadata": {},
      "outputs": [],
      "source": [
        "So far, the model seems to be doing well in learning from the training dataset. But what happens when the model encounters new data from the testing dataset? When the model generates new values from the test data, we see the distribution of the predicted values is much different from the actual target values.\n"
      ]
    },
    {
      "cell_type": "code",
      "id": "f0654c46-b4cd-4be2-9c1c-375627d35222",
      "metadata": {},
      "outputs": [],
      "source": [
        "Title='Distribution  Plot of  Predicted Value Using Test Data vs Data Distribution of Test Data'\nDistributionPlot(y_test,yhat_test,\"Actual Values (Test)\",\"Predicted Values (Test)\",Title)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a45eafd6-16df-4c3e-ad56-a9f4dcdc2f7f",
      "metadata": {},
      "outputs": [],
      "source": [
        "**Figure 2:** Plot of predicted value using the test data compared to the actual values of the test data.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "52e3ad8b-9a4b-400d-a3ed-a07155cdf757",
      "metadata": {},
      "outputs": [],
      "source": [
        "<p>Comparing Figure 1 and Figure 2, it is evident that the distribution of the test data in Figure 1 is much better at fitting the data. This difference in Figure 2. Let's see if polynomial regression also exhibits a drop in the prediction accuracy when analysing the test dataset.</p>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4de7f587-e5de-4853-988a-612a99ad7f20",
      "metadata": {},
      "outputs": [],
      "source": [
        "***Overfitting***\n\n<p>Overfitting occurs when the model fits the noise, but not the underlying process. Therefore, when testing your model using the test set, your model does not perform as well since it is modelling noise, not the underlying process that generated the relationship. Let's create a degree 5 polynomial model.</p>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "da3e9023-900c-4df5-9ebc-8706ef0a7a11",
      "metadata": {},
      "outputs": [],
      "source": [
        "Let's use 75 percent of the data for training and the rest for testing:\n"
      ]
    },
    {
      "cell_type": "code",
      "id": "626e8496-a621-43ac-8ec2-3b3ffa879979",
      "metadata": {},
      "outputs": [],
      "source": [
        "Xp_train, Xp_test, yp_train, yp_test = train_test_split(X, y, test_size=0.25, shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "154adaf0-94c1-45b4-9958-56c6015f0aa5",
      "metadata": {},
      "outputs": [],
      "source": [
        "We will perform a degree 5 polynomial transformation on the feature <b>'doge_avg_price'</b>.\n"
      ]
    },
    {
      "cell_type": "code",
      "id": "852f06e4-6757-4a9c-a0e5-1821c6ee83a4",
      "metadata": {},
      "outputs": [],
      "source": [
        "pr = PolynomialFeatures(degree=5)\nx_train_pr = pr.fit_transform(Xp_train[['doge_avg_price']])\nx_test_pr = pr.fit_transform(Xp_test[['doge_avg_price']])\npr"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ddf50797-89a6-496d-8afd-924808e9dfa0",
      "metadata": {},
      "outputs": [],
      "source": [
        "Now, let's create a Linear Regression model \"poly\" and train it.\n"
      ]
    },
    {
      "cell_type": "code",
      "id": "0f7e3530-33ae-45b3-a3b9-dbab8b3137c4",
      "metadata": {},
      "outputs": [],
      "source": [
        "poly = LinearRegression()\npoly.fit(x_train_pr, yp_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2d78c14e-dfeb-4c45-88bb-a50a3cc48ded",
      "metadata": {},
      "outputs": [],
      "source": [
        "We can see the output of our model using the method \"predict.\" We assign the values to \"yhat\".\n"
      ]
    },
    {
      "cell_type": "code",
      "id": "6abdc819-552d-4863-93bc-aaea68c30928",
      "metadata": {},
      "outputs": [],
      "source": [
        "yhat = poly.predict(x_test_pr)\nyhat[0:5]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7a84e891-9483-46ff-8c43-95db6f3ed24b",
      "metadata": {},
      "outputs": [],
      "source": [
        "Let's take the first five predicted values and compare it to the actual targets.\n"
      ]
    },
    {
      "cell_type": "code",
      "id": "ea378b04-d096-4344-a18a-432a4a16b2c3",
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Predicted values:\", yhat[0:4])\nprint(\"True values:\", yp_test[0:4].values)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "287b32b8-3861-4613-a02c-8dc1c7b2279e",
      "metadata": {},
      "outputs": [],
      "source": [
        "$R^2$ of the training data:\n"
      ]
    },
    {
      "cell_type": "code",
      "id": "02ed2909-8be8-43ff-84f6-8d0ac1ce3f8c",
      "metadata": {},
      "outputs": [],
      "source": [
        "poly.score(x_train_pr, yp_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "956651af-edd6-4f4c-9668-304ac6591360",
      "metadata": {},
      "outputs": [],
      "source": [
        "$R^2$ of the test data:\n"
      ]
    },
    {
      "cell_type": "code",
      "id": "a3d505b6-66c5-4167-bb7d-3297fdc0edfe",
      "metadata": {},
      "outputs": [],
      "source": [
        "poly.score(x_test_pr, yp_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dfd639f7-2f02-48e9-8989-be86821e4ca8",
      "metadata": {},
      "outputs": [],
      "source": [
        "We see the $R^2$ for the training data is 0.48 while the $R^2$ on the test data was 0.72.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "60c6d1af-42f0-46d3-a246-57bfeabb66c0",
      "metadata": {},
      "outputs": [],
      "source": [
        "Let's see how the R^2 changes on the test data for different order polynomials and then plot the results:\n"
      ]
    },
    {
      "cell_type": "code",
      "id": "edef3d4b-cca7-4a66-93f6-ea8ecc99b9af",
      "metadata": {},
      "outputs": [],
      "source": [
        "Rsqu_test = []\n\norder = [1, 2, 3, 4, 5, 6, 7, 8, 9]\nfor n in order:\n    pr = PolynomialFeatures(degree=n)\n    \n    x_train_pr = pr.fit_transform(Xp_train[['doge_avg_price']])\n\n    x_test_pr = pr.fit_transform(Xp_test[['doge_avg_price']])\n\n    lr.fit(x_train_pr, yp_train)\n\n    Rsqu_test.append(lr.score(x_test_pr, yp_test))\n\nplt.plot(order, Rsqu_test)\nplt.xlabel('order')\nplt.ylabel('R^2')\nplt.title('R^2 Using Test Data')\nplt.text(3, 0.75, 'Maximum R^2 ')    "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a9e3aed4-4270-4eb3-8323-59ce64fc89e8",
      "metadata": {},
      "outputs": [],
      "source": [
        "We see the $R^2$ gradually increases until an order 5 polynomial is used. Then, the $R^2$ decreases.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "378ae65a-1e04-42ea-b52f-d3070afbb8eb",
      "metadata": {},
      "outputs": [],
      "source": [
        "Let's try to visualize the polynomial regression of `degree=2`"
      ]
    },
    {
      "cell_type": "code",
      "id": "50bd5c1d-e6a0-4ddd-9669-9edfecc62dfd",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate some sample data\nx_ = np.linspace(0, 10, 100)\ny_ = np.sin(x_) + np.random.normal(0, 0.1, size=len(x_))\n\n# Split data into training and test sets\nx_train_sample, x_test_sample, y_train_sample, y_test_sample = train_test_split(x_, y_, test_size=0.2)\n\n# Create polynomial features for training and test sets\npoly = PolynomialFeatures(degree=2)\nX_train_poly = poly.fit_transform(x_train_sample.reshape(-1, 1))\nX_test_poly = poly.transform(x_test_sample.reshape(-1, 1))\n\n# Fit linear regression model to training data\nlin_reg = LinearRegression()\nlin_reg.fit(X_train_poly, y_train_sample)\n\n# Predict on test data and calculate R^2 score\ny_pred_sample = lin_reg.predict(X_test_poly)\nr2_score = lin_reg.score(X_test_poly, y_test_sample)\n\n# Plot the test data and polynomial fit\nplt.scatter(x_test_sample, y_test_sample)\nplt.scatter(x_test_sample, y_pred_sample, color='red')\nplt.title(f\"Polynomial Regression (degree=2)\\nTest R^2 score: {r2_score:.2f}\")\nplt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bdc4bc67-8e26-4edf-bab1-31d3e2b50a98",
      "metadata": {},
      "outputs": [],
      "source": [
        "The following function will be used later and will help you with understanding of polynomial regression."
      ]
    },
    {
      "cell_type": "code",
      "id": "4b836dae-2adb-45b9-9c8d-801d31926ad7",
      "metadata": {},
      "outputs": [],
      "source": [
        "def f(order, test_data):\n    x_train, x_test, y_train, y_test = train_test_split(x_, y_, test_size=0.2)\n    # Create polynomial features for training and test sets\n    poly = PolynomialFeatures(degree=order)\n    X_train_poly = poly.fit_transform(x_train.reshape(-1, 1))\n    X_test_poly = poly.transform(x_test.reshape(-1, 1))\n\n    # Fit linear regression model to training data\n    lin_reg = LinearRegression()\n    lin_reg.fit(X_train_poly, y_train)\n\n    # Predict on test data and calculate R^2 score\n    y_pred = lin_reg.predict(X_test_poly)\n    r2_score = lin_reg.score(X_test_poly, y_test)\n\n    # Plot the test data and polynomial fit\n    plt.scatter(x_test, y_test)\n    plt.scatter(x_test, y_pred, color='red')\n    plt.title(f\"Polynomial Regression (degree=2)\\nTest R^2 score: {r2_score:.2f}\")\n    plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "89732a7e-06db-44cb-9aff-e6df280abd0e",
      "metadata": {},
      "outputs": [],
      "source": [
        "The following interface allows you to experiment with different polynomial orders and different amounts of data.\n"
      ]
    },
    {
      "cell_type": "code",
      "id": "9082df03-892d-4c90-88e1-d46c2c9c6097",
      "metadata": {},
      "outputs": [],
      "source": [
        "interact(f, order=(0, 6, 1), test_data=(0.05, 0.95, 0.05))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0dfd8cb9-85cd-4a7d-8f51-9f75cc7f2a2b",
      "metadata": {},
      "outputs": [],
      "source": [
        "<div class=\"alert alert-danger alertdanger\" style=\"margin-top: 20px\">\n  <b style=\"font-size: 2em; font-weight: bold;\"> Question  #4a: </b>\n\n  <b>We can perform polynomial transformations with more than one feature. Create a \"PolynomialFeatures\" object \"pr1\" of degree two.</b>\n    \n</div>\n\n"
      ]
    },
    {
      "cell_type": "code",
      "id": "43983771-d23b-4234-a848-c230b17c483d",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Write your code below and press Shift+Enter to execute"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ea530aad-6583-4494-a31f-735fbdaf2898",
      "metadata": {},
      "outputs": [],
      "source": [
        "<details><summary>Click here for the solution</summary>\n\n```python\npr1=PolynomialFeatures(degree=2)\n\n```\n\n</details>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "da41919c-d55c-4512-8c66-334dd840d249",
      "metadata": {},
      "outputs": [],
      "source": [
        "<div class=\"alert alert-danger alertdanger\" style=\"margin-top: 20px\">\n  <b style=\"font-size: 2em; font-weight: bold;\"> Question  #4b: </b>\n\n  <b>Transform the training and testing samples for the features 'doge_avg_price', 'eth_avg_price' and 'xrp_avg_price'. Hint: use the method `fit_transform`.</b>\n    \n</div>\n"
      ]
    },
    {
      "cell_type": "code",
      "id": "930d823f-1ffa-408c-b2fc-1f9851937916",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Write your code below and press Shift+Enter to execute "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1329ede4-b093-4ff0-8689-714197185063",
      "metadata": {},
      "outputs": [],
      "source": [
        "<details><summary>Click here for the solution</summary>\n\n```python\nx_train_pr1 = pr1.fit_transform(Xp_train[['doge_avg_price', 'eth_avg_price', 'xrp_avg_price']])\nx_test_pr1=pr1.fit_transform(Xp_test[['doge_avg_price', 'eth_avg_price', 'xrp_avg_price']])\n\n\n```\n\n</details>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f8909b0c-7b78-4f6d-aa19-9820a0ec5e03",
      "metadata": {},
      "outputs": [],
      "source": [
        "<!-- The answer is below:\n\nx_train_pr1=pr.fit_transform(x_train[['horsepower', 'curb-weight', 'engine-size', 'highway-mpg']])\nx_test_pr1=pr.fit_transform(x_test[['horsepower', 'curb-weight', 'engine-size', 'highway-mpg']])\n\n-->\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8aff3656-bcbf-4bec-8941-ae925ac150ec",
      "metadata": {},
      "outputs": [],
      "source": [
        "<div class=\"alert alert-danger alertdanger\" style=\"margin-top: 20px\">\n  <b style=\"font-size: 2em; font-weight: bold;\"> Question  #4c: </b>\n\n  <b>How many dimensions does the new feature have? Hint: use the attribute \"shape\".</b>\n    \n</div>\n"
      ]
    },
    {
      "cell_type": "code",
      "id": "1115a5b4-ba62-4ae0-aff1-1f459186a975",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Write your code below and press Shift+Enter to execute "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4e92858b-6744-4175-8136-39230e58c8f5",
      "metadata": {},
      "outputs": [],
      "source": [
        "<details><summary>Click here for the solution</summary>\n\n```python\nx_train_pr1.shape \n\n\n```\n\n</details>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5f36e9ee-9f9c-40ce-be33-6ad38f3713a9",
      "metadata": {},
      "outputs": [],
      "source": [
        "<div class=\"alert alert-danger alertdanger\" style=\"margin-top: 20px\">\n  <b style=\"font-size: 2em; font-weight: bold;\"> Question  #4d:</b>\n\n  <b>Create a linear regression model \"poly1\". Train the object using the method \"fit\" using the polynomial features.</b>\n    \n</div>\n"
      ]
    },
    {
      "cell_type": "code",
      "id": "d45b77d4-d21e-4d1d-8175-3d846262f7e9",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Write your code below and press Shift+Enter to execute "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b32165bb-d4f5-428c-98d2-b97e49c9709d",
      "metadata": {},
      "outputs": [],
      "source": [
        "<details><summary>Click here for the solution</summary>\n\n```python\npoly1=LinearRegression().fit(x_train_pr1,yp_train)\n\n\n```\n\n</details>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a0d83d4d-d73f-46fe-b135-2067a03dbf3d",
      "metadata": {},
      "outputs": [],
      "source": [
        "<div class=\"alert alert-danger alertdanger\" style=\"margin-top: 20px\">\n  <b style=\"font-size: 2em; font-weight: bold;\"> Question  #4e:</b>\n\n  <b>Use the method  \"predict\" to predict an output on the polynomial features, then use the function \"DistributionPlot\" to display the distribution of the predicted test output vs. the actual test data.</b>\n    \n</div>\n"
      ]
    },
    {
      "cell_type": "code",
      "id": "a0644842-dbca-4651-8884-ee8fcfe1e671",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Write your code below and press Shift+Enter to execute"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "672b0ce2-71e6-4bd4-97a5-4690be4c0ff1",
      "metadata": {},
      "outputs": [],
      "source": [
        "<details><summary>Click here for the solution</summary>\n\n```python\nyhat_test1=poly1.predict(x_test_pr1)\n\nTitle='Distribution  Plot of  Predicted Value Using Test Data vs Data Distribution of Test Data'\n\nDistributionPlot(y_test, yhat_test1, \"Actual Values (Test)\", \"Predicted Values (Test)\", Title)\n\n```\n\n</details>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "da28e295-fb4b-471e-b59e-ade8706a7ef5",
      "metadata": {},
      "outputs": [],
      "source": [
        "## Ridge Regression\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "064c1ea2-c42b-488b-af12-0f5286b72659",
      "metadata": {},
      "outputs": [],
      "source": [
        "Ridge regression is a type of regularized linear regression method that can help to prevent overfitting in a model. It is often used when there are a large number of variables in the dataset and multicollinearity (correlation among predictor variables) is present.\n\nIn ridge regression, a penalty term is added to the least squares objective function, which shrinks the estimated coefficients towards zero. The amount of shrinkage is controlled by a hyperparameter, called the regularization parameter or lambda (λ), which determines the strength of the penalty. A higher value of λ will lead to more shrinkage and smaller coefficient estimates.\n\nThe ridge regression coefficient estimates are obtained by minimizing the following objective function:\n\n$$\\min_{\\beta_0,\\beta} \\sum_{i=1}^{n} (y_i - \\beta_0 - \\sum_{j=1}^{p} \\beta_j x_{ij})^2 + \\lambda \\sum_{j=1}^{p} \\beta_j^2$$\n\nwhere:\n\n* $y_i$ is the observed response for the i-th observation.\n* $x_{ij}$ is the j-th predictor variable for the i-th observation.\n* $\\beta_0$ and $\\beta_j$ are the intercept and coefficient estimates, respectively.\n* p is the number of predictor variables.\n* n is the number of observations.\nThe first term in the objective function is the ordinary least squares (OLS) term, and the second term is the penalty term. The goal of ridge regression is to find the values of $\\beta_0$ and $\\beta_j$ that minimize the sum of these two terms.\n\nRidge regression can be particularly useful when dealing with multicollinearity, which can make the OLS coefficient estimates unstable and difficult to interpret. By adding a penalty term to the objective function, ridge regression can reduce the variance of the coefficient estimates, making them more stable and easier to interpret.\n\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "efc7aa04-9415-4805-b8a2-f074748dc044",
      "metadata": {},
      "outputs": [],
      "source": [
        "Let's perform a degree two polynomial transformation on our data.\n"
      ]
    },
    {
      "cell_type": "code",
      "id": "01aadb93-feee-417a-bc66-98df9f1d967b",
      "metadata": {},
      "outputs": [],
      "source": [
        "pr=PolynomialFeatures(degree=2)\nx_train_pr=pr.fit_transform(Xp_train[['doge_avg_price', 'eth_avg_price', 'xrp_avg_price']])\nx_test_pr=pr.fit_transform(Xp_test[['doge_avg_price', 'eth_avg_price', 'xrp_avg_price']])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5039df50-657e-4003-9195-7d2eaf99da63",
      "metadata": {},
      "outputs": [],
      "source": [
        "Let's create a Ridge regression object, setting the regularization parameter (alpha) to 0.1\n"
      ]
    },
    {
      "cell_type": "code",
      "id": "914f318d-0633-406f-90b2-05f70350edda",
      "metadata": {},
      "outputs": [],
      "source": [
        "RigeModel=Ridge(alpha=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0c432a10-5a07-47d5-83ba-9f417c9d1650",
      "metadata": {},
      "outputs": [],
      "source": [
        "Like regular regression, you can fit the model using the method <b>fit</b>.\n"
      ]
    },
    {
      "cell_type": "code",
      "id": "bdfaeb2c-00f0-462c-a216-570c8f1baa62",
      "metadata": {},
      "outputs": [],
      "source": [
        "RigeModel.fit(x_train_pr, yp_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "70105606-abbb-4592-ab84-f36bb7fc385a",
      "metadata": {},
      "outputs": [],
      "source": [
        "Similarly, you can obtain a prediction:\n"
      ]
    },
    {
      "cell_type": "code",
      "id": "ddedbb0e-9f19-4d83-b2f8-34e9d86767dc",
      "metadata": {},
      "outputs": [],
      "source": [
        "yhat = RigeModel.predict(x_test_pr)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "37ae6dfd-4d36-4e4b-b275-c5eaf15581e7",
      "metadata": {},
      "outputs": [],
      "source": [
        "Let's compare the first five predicted samples to our test set:\n"
      ]
    },
    {
      "cell_type": "code",
      "id": "8ef65689-cf5a-47f8-92c2-9e2232d5fa44",
      "metadata": {},
      "outputs": [],
      "source": [
        "print('Predicted:', yhat[0:4])\nprint('Test set :', y_test[0:4].values)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c79223c0-d1c9-4e65-9c0b-dbd3175bb0d9",
      "metadata": {},
      "outputs": [],
      "source": [
        "We want to select the value of alpha that minimizes the test error. To do so, we can use a loop. We will also create a progress bar to see how many iterations we have completed.\n"
      ]
    },
    {
      "cell_type": "code",
      "id": "fcc3a056-acb7-4af6-8895-c967da41cae1",
      "metadata": {},
      "outputs": [],
      "source": [
        "Rsqu_test = []\nRsqu_train = []\ndummy1 = []\nAlpha = 10 * np.array(range(0,1000))\npbar = tqdm(Alpha)\n\nfor alpha in pbar:\n    RigeModel = Ridge(alpha=alpha) \n    RigeModel.fit(x_train_pr, yp_train)\n    test_score, train_score = RigeModel.score(x_test_pr, yp_test), RigeModel.score(x_train_pr, yp_train)\n\n    pbar.set_postfix({\"Test Score\": test_score, \"Train Score\": train_score})\n\n    Rsqu_test.append(test_score)\n    Rsqu_train.append(train_score)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e39b4c5b-ccba-4cb4-ac43-8ae6ca5ebd39",
      "metadata": {},
      "outputs": [],
      "source": [
        "We can plot out the value of $R^2$ for different alphas:\n"
      ]
    },
    {
      "cell_type": "code",
      "id": "0fe2307d-d8b8-4e44-95a6-53e7b24429f6",
      "metadata": {},
      "outputs": [],
      "source": [
        "width = 12\nheight = 10\nplt.figure(figsize=(width, height))\n\nplt.plot(Alpha,Rsqu_test, label='Validation data  ')\nplt.plot(Alpha,Rsqu_train, 'r', label='Training Data ')\nplt.xlabel('alpha')\nplt.ylabel('R^2')\nplt.legend()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0a488af8-139b-4b9b-a6f2-81e7840514dd",
      "metadata": {},
      "outputs": [],
      "source": [
        "**Figure 4**: The blue line represents the $R^2$ of the validation data, and the red line represents the $R^2$ of the training data. The x-axis represents the different values of Alpha.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a2b6dd06-f128-4d32-90b5-90201e85ab5d",
      "metadata": {},
      "outputs": [],
      "source": [
        "Here the model is built and tested on the same data, so the training and test data are the same.\n\nThe **red line in Figure 4** represents the $R^2$ of the training data. As alpha increases the $R^2$ for train remains the same.\n\nThe **blue line** represents the $R^2$ on the validation data. As the value for alpha increases, the $R^2$ increases.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bac40b34-c9cc-4b50-8c48-967b33a397eb",
      "metadata": {},
      "outputs": [],
      "source": [
        "<div class=\"alert alert-danger alertdanger\" style=\"margin-top: 20px\">\n  <b style=\"font-size: 2em; font-weight: bold;\"> Question  #5:</b>\n\n  <b>Perform Ridge regression. Calculate the $R^2$ using the polynomial features, use the training data to train the model and use the test data to test the model. The parameter alpha should be set to 10.</b>\n    \n</div>\n"
      ]
    },
    {
      "cell_type": "code",
      "id": "75cfbd43-e929-4a43-be6c-9cd4d22cfd5d",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Write your code below and press Shift+Enter to execute "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f74bba95-9eec-4ba3-b2ac-c075b0f0af48",
      "metadata": {},
      "outputs": [],
      "source": [
        "<details><summary>Click here for the solution</summary>\n\n```python\nRigeModel = Ridge(alpha=10) \nRigeModel.fit(x_train_pr, yp_train)\nRigeModel.score(x_test_pr, yp_test)\n\n```\n\n</details>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7c84fec9-f9ae-45b8-a344-ebe7c3691400",
      "metadata": {},
      "outputs": [],
      "source": [
        "## Grid Search\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5f7cda29-0602-434e-bdad-27205889f569",
      "metadata": {},
      "outputs": [],
      "source": [
        "The term alpha is a hyperparameter. Sklearn has the class <b>GridSearchCV</b> to make the process of finding the best hyperparameter simpler.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8e7292fa-de57-48ea-b104-d1bc438ad62e",
      "metadata": {},
      "outputs": [],
      "source": [
        "**When to use GridSearchCV?**\n\nYou can use GridSearchCV when you want to:\n* Select the optimal hyperparameters for your machine learning model.\n* Automate the process of hyperparameter tuning.\n* Search over a large space of possible hyperparameters.\n* Obtain a more robust and accurate model."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "85a2f010-ab4a-4d3c-a1c9-27d77f3e679c",
      "metadata": {},
      "outputs": [],
      "source": [
        "For more info on GridSearchCV follow:\n   * https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html\n   * https://towardsdatascience.com/gridsearchcv-for-beginners-db48a90114ee"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5f9d9151-7ef0-4305-a415-7ded6881aecf",
      "metadata": {},
      "outputs": [],
      "source": [
        "Let's create a list with a dictionary of parameter values(we will need it for GridSearchCV):\n"
      ]
    },
    {
      "cell_type": "code",
      "id": "87f4dcd6-4719-4e58-b505-eaea52b1283e",
      "metadata": {},
      "outputs": [],
      "source": [
        "parameters1= [{'alpha': [0.001,0.1,1, 10, 100, 1000, 10000, 100000, 100000]}]\nparameters1"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4f7cbd4f-375f-42d0-bd68-6995cfc28557",
      "metadata": {},
      "outputs": [],
      "source": [
        "Now, let's create a Ridge regression object:\n"
      ]
    },
    {
      "cell_type": "code",
      "id": "0e2c0ab6-6b84-4d69-864b-db480d6c6d39",
      "metadata": {},
      "outputs": [],
      "source": [
        "RR=Ridge()\nRR"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "71ef4adc-2d30-4cfd-a55e-b8154ef553d0",
      "metadata": {},
      "outputs": [],
      "source": [
        "Also let's create a ridge grid search object:\n"
      ]
    },
    {
      "cell_type": "code",
      "id": "02f0ab4d-0bcf-4549-b641-fd1f91febfaa",
      "metadata": {},
      "outputs": [],
      "source": [
        "Grid1 = GridSearchCV(RR, parameters1,cv=4)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9317e5d0-fbe8-4120-848a-65df3f5e1547",
      "metadata": {},
      "outputs": [],
      "source": [
        "\nFit the model:\n"
      ]
    },
    {
      "cell_type": "code",
      "id": "9d2befbc-38a3-45ba-bad0-83122c9a7a90",
      "metadata": {},
      "outputs": [],
      "source": [
        "Grid1.fit(X[['doge_avg_price', 'eth_avg_price', 'xrp_avg_price']],y)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "df9cd7cf-cbbf-4b89-b4de-ea8589a17c38",
      "metadata": {},
      "outputs": [],
      "source": [
        "The object finds the best parameter values on the validation data. We can obtain the estimator with the best parameters and assign it to the variable BestRR as follows:\n"
      ]
    },
    {
      "cell_type": "code",
      "id": "218bd6d9-1dd2-4993-9ea3-dffa60df3d92",
      "metadata": {},
      "outputs": [],
      "source": [
        "BestRR=Grid1.best_estimator_\nBestRR"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "deb08a33-3402-4cf2-b550-88b31c31e1ad",
      "metadata": {},
      "outputs": [],
      "source": [
        "We can see that the best parameter is `alpha=0.001`"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4867d867-5356-47ce-ad51-8c9604c21f89",
      "metadata": {},
      "outputs": [],
      "source": [
        "Let's test our model on the test data:\n"
      ]
    },
    {
      "cell_type": "code",
      "id": "eba5e7d7-1e43-4173-8a97-74056d11fcde",
      "metadata": {},
      "outputs": [],
      "source": [
        "BestRR.score(X_test[['doge_avg_price', 'eth_avg_price', 'xrp_avg_price']], y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "300b60e0-5510-4e0d-a7df-c8374fa2212f",
      "metadata": {},
      "outputs": [],
      "source": [
        "Let's use this model to predict values and then visualize the results"
      ]
    },
    {
      "cell_type": "code",
      "id": "d04050e9-ad5f-4c20-ba64-96e40528a0f6",
      "metadata": {},
      "outputs": [],
      "source": [
        "rr_predicted = BestRR.predict(X_test[['doge_avg_price', 'eth_avg_price', 'xrp_avg_price']])\nrr_predicted"
      ]
    },
    {
      "cell_type": "code",
      "id": "e93d3a72-e5f2-4959-86a0-42dc899a86a8",
      "metadata": {},
      "outputs": [],
      "source": [
        "data = {'Predicted': rr_predicted, 'Actual': y_test}\ndf = pd.DataFrame(data)\n\n# Use seaborn to create a scatter plot with a regression line\nsns.lmplot(x='Predicted', y='Actual', data=df, scatter_kws={'color': 'blue'}, line_kws={'color': 'red'})\n\n# Display the plot\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c63bce9a-79f0-4180-a138-4df56814055c",
      "metadata": {},
      "outputs": [],
      "source": [
        "As you can see using `GridSearchCV` we were able to find optimal alpha and improve our model."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "98bd2111-4331-4f90-8a6a-1f316771807e",
      "metadata": {},
      "outputs": [],
      "source": [
        "# **Thank you for completing Lab 5!**\n\n## Authors\n\n<a href=\"https://author.skills.network/instructors/nazar_kohut\">Nazar Kohut</a>\n\n<a href=\"https://author.skills.network/instructors/yaroslav_vyklyuk_2?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkGuidedProjectsIBMSkillsNetworkGPXX0QGDEN2306-2023-01-01\">Prof. Yaroslav Vyklyuk, DrSc, PhD</a>\n\n<a href=\"https://author.skills.network/instructors/mariya_fleychuk?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkGuidedProjectsIBMSkillsNetworkGPXX0QGDEN2306-2023-01-01\">Prof. Mariya Fleychuk, DrSc, PhD</a>\n\n\n## Change Log\n\n| Date (YYYY-MM-DD) | Version | Changed By   | Change Description                                         |\n| ----------------- | ------- | -------------| ---------------------------------------------------------- |\n|     2023-25-03    |   1.0   | Nazar Kohut  | Lab created                                                |\n\n<hr>\n\n## <h3 align=\"center\"> © IBM Corporation 2023. All rights reserved. <h3/>"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": ""
    },
    "kernelspec": {
      "name": "",
      "display_name": ""
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
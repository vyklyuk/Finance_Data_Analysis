{"cells":[{"cell_type":"markdown","id":"c2dc6e60-66bf-4252-8980-40fd8c6188cd","metadata":{},"outputs":[],"source":["\u003ccenter\u003e\n","    \u003cimg src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/assets/logos/SN_web_lightmode.png\" width=\"500\" alt=\"cognitiveclass.ai logo\"\u003e\n","\u003c/center\u003e\n","\n","#  **Investigation of MATIC/BUSD exchange rate dynamic,  calculation and analysis of separate  technical financial indicators of cryptocurrency market (ATR, OBV, RSI, AD)**\n","\n","## **Lab 5. Model Evaluation and Refinement**\n","\n","## **The tasks**\n","* To make model evaluation,using functions for:  Plotting, Training and Testing, Cross-Validation\n","* To analyze Overfitting, Underfitting and select the Model;\n","* To built Ridge Regression;\n","* to use Grid Search.\n","\n","Estimated time needed: **30** minutes\n","\n","## **Objectives**\n","\n","After completing this lab you will be able to:\n","\n","* evaluate and Refine Prediction Models;\n","* use Grid Search, Cross-Validation;\n","* built Ridge Regression;\n","* to use Grid Search.\n"]},{"cell_type":"markdown","id":"5514475e-d2e3-48d0-a7d9-1080a0cfeed6","metadata":{},"outputs":[],"source":["## **Table of Contents**\n","\n","\u003cdiv class=\"alert alert-block alert-info\" style=\"margin-top: 20px\"\u003e\n","\u003col\u003e\n","    \u003cli\u003eImport Data\u003c/li\u003e\n","    \u003cli\u003eModel Evaluation\u003c/li\u003e\n","    \u003cul\u003e\n","        \u003cli\u003eFunctions for Plotting\u003c/li\u003e\n","        \u003cli\u003eTraining and Testing\u003c/li\u003e\n","        \u003cli\u003eCross-Validation\u003c/li\u003e\n","    \u003c/ul\u003e\n","    \u003cli\u003eOverfitting, Underfitting and Model Selection\u003c/li\u003e\n","    \u003cul\u003e\n","       \u003cli\u003eOverfitting\u003c/li\u003e \n","    \u003c/ul\u003e\n","    \u003cli\u003eRidge Regression\u003c/li\u003e\n","    \u003cli\u003eGrid Search\u003c/li\u003e\n","    \u003cli\u003eSources\u003c/li\u003e\n","\u003c/ol\u003e\n","\n","\u003c/div\u003e\n","\n","\u003chr\u003e\n"]},{"cell_type":"markdown","id":"7d4046a1-87cc-454a-97f9-60843fbc8f8e","metadata":{},"outputs":[],"source":["## **Dataset Description**\n","\n","### **Files**\n","* #### **MATICBUSD_trades_1m_preprocessed.csv** - the file contains historical changes of the pair **MATIC/BUSD** and ATR, OBV, RSI, AD indicators for the period from 11/11/2022 to 12/29/2022 with an aggregation time of 1 minute. **MATIC/BUSD** - the exchange rate of **MATIC** cryptocurrency to **BUSD** cryptocurrency\n","\n","### **Columns**\n","\n","* #### `Ts` - the timestamp of the record\n","* #### `Open` -  the price of the asset at the beginning of the trading period\n","* #### `High` -  the highest price of the asset during the trading period\n","* #### `Low` - the lowest price of the asset during the trading period.\n","* #### `Close` - the price of the asset at the end of the trading period\n","* #### `Volume` - the total number of shares or contracts of a particular asset that are traded during a given period\n","* #### `Rec_count` -  the number of individual trades or transactions that have been executed during a given time period\n","* #### `Avg_price` - the average price at which a particular asset has been bought or sold during a given period\n","* #### `ATR` - average true range indicator\n","* #### `OBV` - on-balance volume indicator\n","* #### `RSI` - relative strength index indicator\n","* #### `AD` - accumulation / distribution indicator\n"]},{"cell_type":"markdown","id":"a9c4ea87-db54-465d-9866-75e7b0c1d081","metadata":{},"outputs":[],"source":["# **1. Import Data**\n"]},{"cell_type":"markdown","id":"4cf40e10-0c25-445f-b236-013a3198cda3","metadata":{},"outputs":[],"source":["Run the following cell to install required libraries:\n"]},{"cell_type":"code","id":"754ef9ca-a78c-49cd-bd63-ba6f210514d3","metadata":{},"outputs":[],"source":["# install specific version of libraries used in lab\n# ! conda install -q -y pandas\n! conda install -q -y numpy\n! conda install -q -y -c anaconda scikit-learn\n# ! conda install -q -y -c anaconda ipywidgets\n# ! conda install -q -y tqdm"]},{"cell_type":"code","id":"8ea085ec-481e-4ab8-bc1e-1f9217dd697d","metadata":{},"outputs":[],"source":["import pandas as pd\nimport numpy as np\nfrom ipywidgets import interact, interactive, fixed, interact_manual\n\nfrom sklearn.linear_model import LinearRegression, Ridge\nfrom sklearn.model_selection import cross_val_score, cross_val_predict, GridSearchCV, train_test_split\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.metrics import r2_score, mean_squared_error\nfrom sklearn import set_config\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom tqdm import tqdm\nimport warnings\n\nwarnings.filterwarnings(\"ignore\")\nset_config(\"diagram\")\n\n# setting hyperparameters\nWIDTH, HEIGHT = 12, 10"]},{"cell_type":"markdown","id":"cd80e35c-423f-4008-a8ab-f41be90aca1f","metadata":{},"outputs":[],"source":["This dataset was hosted \u003ca href=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMSkillsNetwork-GPXX08XFEN/MATICBUSD_trades_1m_preprocessed.csv\"\u003eHERE\u003c/a\u003e\n"]},{"cell_type":"code","id":"f67805fa-d35b-4602-9cc6-3719cf8b43a8","metadata":{},"outputs":[],"source":["path = \"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMSkillsNetwork-GPXX08XFEN/MATICBUSD_trades_1m_preprocessed.csv\""]},{"cell_type":"markdown","id":"53035db1-5133-489a-bd6e-848eefb9a288","metadata":{},"outputs":[],"source":["Let's read and print the dataset\n"]},{"cell_type":"code","id":"ad838364-27ed-43c2-aaaf-332a7c571cce","metadata":{},"outputs":[],"source":["df = pd.read_csv(path)\ndf.head()"]},{"cell_type":"markdown","id":"d5e33eac-db21-45dd-8548-5c31e37c5092","metadata":{},"outputs":[],"source":["We need to drop first 15 `NaN`'s\n"]},{"cell_type":"code","id":"0ead671d-7ec5-40ca-bbad-d2d2ebaafad1","metadata":{},"outputs":[],"source":["df = df.dropna()"]},{"cell_type":"markdown","id":"12b72efd-49e6-486a-b13c-ec749f08ebf3","metadata":{},"outputs":[],"source":["# **2. Model evaluation**\n"]},{"cell_type":"markdown","id":"28f6c6e2-5ca7-4cbc-9e34-624471a31416","metadata":{},"outputs":[],"source":["## **Functions for Plotting**\n"]},{"cell_type":"markdown","id":"f4c0f08d-3cd5-4349-bfcf-f816d3066241","metadata":{},"outputs":[],"source":["Let's define 2 functions: \u003ccode\u003edist_plot\u003c/code\u003e (for plotting distributions) and \u003ccode\u003epoly_plot\u003c/code\u003e (for plotting Polynomial Regression)\n"]},{"cell_type":"code","id":"6a1244a2-ac3e-4ac1-bf68-4b72a9db68b6","metadata":{},"outputs":[],"source":["def dist_plot(y: pd.Series, y_pred: pd.Series, title: str) -\u003e None:\n    \"\"\"\n    Plots distribution plot of `y` and `y_pred` with title = `title`\n    \n    Parameters\n    ----------\n    y: pd.Series\n        True values\n    y_pred: pd.Series\n        Predicted values\n    title: str\n        The title of plot\n    \"\"\"\n    plt.figure(figsize=(WIDTH, HEIGHT))\n    \n    temp_df = pd.DataFrame({\"y\": y, \"y_pred\": y_pred})\n    temp_df.plot.kde(figsize=(WIDTH, HEIGHT))\n\n    plt.title(title)\n    plt.xlabel(\"Price (in BUSD)\", labelpad=0.02)\n    plt.ylabel(\"Proportion\")\n\n    plt.show()"]},{"cell_type":"code","id":"8f0da6c4-3a2f-43cd-b044-93fe01b45d21","metadata":{},"outputs":[],"source":["def poly_plot(x_train: pd.Series, x_test: pd.Series, y_train: pd.Series, y_test: pd.Series, lr: LinearRegression, poly_transform: PolynomialFeatures, indicator: str) -\u003e None:\n    \"\"\"\n    Plots plot of polynomial regression\n    \n    Parameters\n    ----------\n    x_train: pd.Series\n        Train x\n    x_test: pd.Series\n        Test x\n    y_train: pd.Series\n        Train y\n    y_test: pd.Series\n        Test y\n    lr: sklearn.linear_model.LinearRegression\n        Model of LinearRegression\n    pr: sklearn.preprocessing.PolynomialFeatures\n        Polynomial features\n    indicator: str\n        x-label\n    \"\"\"\n    plt.figure(figsize=(WIDTH, HEIGHT))\n \n    x_max = max([x_train.values.max(), x_test.values.max()])\n    x_min = min([x_train.values.min(), x_test.values.min()])\n    x = np.arange(x_min, x_max, 100)\n\n    plt.plot(x_train, y_train, \"ro\", label=\"Training Data\")\n    plt.plot(x_test, y_test, \"go\", label=\"Test Data\")\n    plt.plot(x, lr.predict(poly_transform.fit_transform(x.reshape(-1, 1))), label=\"Predicted Function\")\n    \n    plt.title(\"Plot of train, test data and predicted function\")\n    plt.xlabel(indicator)\n    plt.ylabel(\"Price\")\n    plt.legend()"]},{"cell_type":"markdown","id":"7d638239-03cf-40fe-b7cc-2592fffaf97f","metadata":{},"outputs":[],"source":["## **Training and Testing**\n","\n","\u003ccenter\u003e\n","    \u003cimg src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMSkillsNetwork-GPXX08XFEN/PartitionTwoSets.svg\" alt=\"train-test-dataset\"\u003e\n","\u003c/center\u003e\n","\n","\u003ccenter\u003eWe split the dataset into 2 parts. One for training, the other for testing\u003c/center\u003e\n"]},{"cell_type":"markdown","id":"c0105a9b-eb17-4c2d-964e-f8293b7f38e8","metadata":{},"outputs":[],"source":["Splitting a dataset into **training** and **testing** datasets is a critical step in developing machine learning models. Here are some reasons why:\n","\n","1. Avoiding overfitting: Machine learning models aim to find patterns in data and learn from them. However, if the model becomes too complex, it may fit the training data too closely, capturing noise in the data rather than generalizing to new data. Splitting the dataset into training and testing sets allows us to assess whether the model is overfitting to the training data.\n","\n","2. Evaluating model performance: Splitting the dataset into training and testing sets allows us to evaluate the performance of the model on new, unseen data. By testing the model on the testing set, we can assess how well the model generalizes to new data and avoid overestimating the model's performance.\n","\n","3. Hyperparameter tuning: Splitting the dataset into training and validation sets allows us to tune the hyperparameters of the model. Hyperparameters are parameters that are not learned from the data but are set before training the model, such as the learning rate or the number of layers in a neural network. By evaluating the model's performance on the validation set, we can tune the hyperparameters to improve the model's performance.\n"]},{"cell_type":"markdown","id":"57b8678d-7d41-4fc9-a848-ab93b901d354","metadata":{},"outputs":[],"source":["**Overfitting** will be discussed a bit later. Let's assign target value to \u003ccode\u003ey_data\u003c/code\u003e\n"]},{"cell_type":"code","id":"664e4413-5af9-4c29-a693-036aad768335","metadata":{},"outputs":[],"source":["y_data = df[\"Avg_price\"]"]},{"cell_type":"markdown","id":"c6ba29e0-dd4b-4dfe-afa4-4e7d14732462","metadata":{},"outputs":[],"source":["Drop price data in dataframe \u003ccode\u003ex_data\u003c/code\u003e:\n"]},{"cell_type":"code","id":"341c1f82-aa9c-48d6-b856-1a4e7d06d4b6","metadata":{},"outputs":[],"source":["x_data = df.drop(\"Avg_price\", axis=1)"]},{"cell_type":"markdown","id":"c2609fce-4e50-4612-88e7-1d1f957a17a8","metadata":{},"outputs":[],"source":["Now, we split our data into training and testing data using the function \u003ccode\u003etrain_test_split\u003c/code\u003e. This function splits arrays or matrices into random train and test subsets.\n"]},{"cell_type":"code","id":"f95c14a7-ce35-441b-8e62-32c090e3265e","metadata":{},"outputs":[],"source":["x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, test_size=0.10, shuffle=False)\n\nprint(\"Number of test samples :\", x_test.shape[0])\nprint(\"Number of training samples:\", x_train.shape[0])"]},{"cell_type":"markdown","id":"0d8478ae-e1d1-4a8d-8f7b-c31a086df8fa","metadata":{},"outputs":[],"source":["The \u003ccode\u003etest_size\u003c/code\u003e parameter sets the proportion of data that is split into the testing set. In the above, the testing set is 10% of the total dataset.\n"]},{"cell_type":"markdown","id":"620e5e5c-27be-4ca0-894e-48f85e625be5","metadata":{},"outputs":[],"source":["\u003cdiv class=\"alert alert-danger alertdanger\" style=\"margin-top: 20px\"\u003e\n","\n","# **Question  #1):**\n","\n","**Use the function `train_test_split` to split up the dataset such that 40% of the data samples will be utilized for testing and `shuffle=False`. Print the shape of train and test $x$**\n","\n","\u003c/div\u003e\n"]},{"cell_type":"code","id":"f6f9ea0b-c879-4133-9982-34bdf890c8ca","metadata":{},"outputs":[],"source":["# Write your code below and press Shift+Enter to execute \n"]},{"cell_type":"markdown","id":"3a6eb66f-3981-4777-b7d3-ba3eb59ddbb3","metadata":{},"outputs":[],"source":["\u003cdetails\u003e\u003csummary\u003eClick here for the solution\u003c/summary\u003e\n","\n","```python\n","x_train1, x_test1, y_train1, y_test1 = train_test_split(x_data, y_data, test_size=0.4, shuffle=False) \n","print(\"Number of test samples :\", x_test1.shape[0])\n","print(\"Number of training samples:\", x_train1.shape[0])\n","```\n","\n","\u003c/details\u003e\n"]},{"cell_type":"markdown","id":"2f243714-aca0-4ea6-a487-d63f3f1d9d56","metadata":{},"outputs":[],"source":["We create a Linear Regression object:\n"]},{"cell_type":"code","id":"a79604b2-072a-412d-9005-b529ddb1378f","metadata":{},"outputs":[],"source":["lre = LinearRegression()\nlre"]},{"cell_type":"markdown","id":"1a3312d2-8bcb-4f33-8b70-ad90a32259eb","metadata":{},"outputs":[],"source":["We fit the model using the feature **\"AD\"**:\n"]},{"cell_type":"code","id":"762ceb94-539b-4da0-8dba-d4689ff946de","metadata":{},"outputs":[],"source":["lre.fit(x_train[[\"AD\"]], y_train)"]},{"cell_type":"markdown","id":"39efeaf3-1569-49e8-9724-2e9bb88ac909","metadata":{},"outputs":[],"source":["Let's calculate the $R^2$ on the test data:\n"]},{"cell_type":"code","id":"cc9049c5-ef32-4451-8a54-1170f4c73451","metadata":{},"outputs":[],"source":["lre.score(x_test[[\"AD\"]], y_test)"]},{"cell_type":"markdown","id":"fd2c48ea-1cb6-49e6-8af4-4398020220b9","metadata":{},"outputs":[],"source":["We can see the $R^2$ of the test dataset is negative. $R^2$ is negative only when the chosen model does not follow the trend of the data, so fits worse than a horizontal line   \n"]},{"cell_type":"code","id":"108eea80-8a7c-45b0-9e53-d62b3bd8070c","metadata":{},"outputs":[],"source":["lre.score(x_train[[\"AD\"]], y_train)"]},{"cell_type":"markdown","id":"d994ba36-9a7c-40e2-aed1-a388d292f2d1","metadata":{},"outputs":[],"source":["As we cannot judge by $R^2$ let's calculate $MSE$ and compare them\n"]},{"cell_type":"code","id":"9d97649d-3079-4140-9b56-ffb6f9c9ecd5","metadata":{},"outputs":[],"source":["mean_squared_error(y_train, lre.predict(x_train[[\"AD\"]]))"]},{"cell_type":"code","id":"48ea3e1b-ba73-4646-889f-df1d4886bd8e","metadata":{},"outputs":[],"source":["mean_squared_error(y_test, lre.predict(x_test[[\"AD\"]]))"]},{"cell_type":"markdown","id":"85e562d1-4868-4814-9d65-5d29ac3729e2","metadata":{},"outputs":[],"source":["$MSE$ calculated from the train dataset is half the $MSE$ calculated from the test dataset what means the loss of train dataset is less than test one\n"]},{"cell_type":"markdown","id":"b6f28955-0008-4d68-98d4-aeedb4c81ca3","metadata":{},"outputs":[],"source":["\u003cdiv class=\"alert alert-danger alertdanger\" style=\"margin-top: 20px\"\u003e\n","\n","# **Question  #2):**\n","    \n","**Split dataset (40% for testing, 60% for training, `shuffle=False`). Create LinearRegression `lre2` and train on the train data. Find the $R^2$ and $MSE$ on the test data using 40% of the dataset for testing.**\n","    \n","\u003c/div\u003e\n"]},{"cell_type":"code","id":"a501220f-4af7-4141-8b7e-fd9c8155e76c","metadata":{},"outputs":[],"source":["# Write your code below and press Shift+Enter to execute \n"]},{"cell_type":"markdown","id":"8bf697ae-d5ba-4943-a47b-2b38058b9fb6","metadata":{},"outputs":[],"source":["\u003cdetails\u003e\u003csummary\u003eClick here for the solution\u003c/summary\u003e\n","\n","```python\n","x_train2, x_test2, y_train2, y_test2 = train_test_split(x_data, y_data, test_size=0.4, shuffle=False)\n","lre2 = LinearRegression()\n","lre2.fit(x_train2[[\"AD\"]], y_train2)\n","print(f'R-squared: {lre2.score(x_test2[[\"AD\"]], y_test2)}, MSE: {mean_squared_error(y_test2, lre2.predict(x_test2[[\"AD\"]]) ) }')\n","```\n","\n","\u003c/details\u003e\n"]},{"cell_type":"markdown","id":"708c3991-a3b4-4500-b247-d874f4cc6052","metadata":{},"outputs":[],"source":["Sometimes you do not have sufficient testing data; as a result, you may want to perform cross-validation. Let's go over several methods that you can use for cross-validation.\n"]},{"cell_type":"markdown","id":"da353257-2004-4a48-a4bd-3a3452142e31","metadata":{},"outputs":[],"source":["## **Cross-Validation**\n"]},{"cell_type":"markdown","id":"fb9e0b6b-4048-47fd-8075-994c65fe0858","metadata":{},"outputs":[],"source":["\u003ccenter\u003e\n","    \u003cimg src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMSkillsNetwork-GPXX08XFEN/1676051591395.jpg\" alt=\"cross-validation\"\u003e\n","\u003c/center\u003e\n","\n","\u003ccenter\u003eThe Example of 4-fold cross-validation\u003c/center\u003e\n"]},{"cell_type":"markdown","id":"810dfd33-2d22-448c-94d7-83f74e0e2e6a","metadata":{},"outputs":[],"source":["**Cross-validation**, sometimes called **rotation estimation** or **out-of-sample testing**, is any of various similar model validation techniques for assessing how the results of a statistical analysis will generalize to an independent data set. **Cross-validation** is a resampling method that uses different portions of the data to test and train a model on different iterations. \n","\n","It is mainly used in settings where the goal is prediction, and one wants to estimate how accurately a predictive model will perform in practice. In a prediction problem, a model is usually given a dataset of known data on which training is run (training dataset), and a dataset of unknown data (or first seen data) against which the model is tested (called the validation dataset or testing set). \n","\n","The goal of **cross-validation** is to test the model's ability to predict new data that was not used in estimating it, in order to flag problems like overfitting or selection bias and to give an insight on how the model will generalize to an independent dataset (i.e., an unknown dataset, for instance from a real problem).\n"]},{"cell_type":"markdown","id":"82ca3084-cca5-483e-8135-dfbd5cdb4ec2","metadata":{},"outputs":[],"source":["We input the object, the feature (**\"AD\"**), and the target data (`y_data`). The parameter `cv` determines the number of folds. In this case, it is 4.\n"]},{"cell_type":"code","id":"7d862557-87d0-4b26-b311-9176baaaedeb","metadata":{},"outputs":[],"source":["R_cross = cross_val_score(lre, x_data[[\"AD\"]], y_data, cv=4, scoring=\"r2\")"]},{"cell_type":"markdown","id":"9bb83eb9-e9d1-4030-869c-8b0418c66dd7","metadata":{},"outputs":[],"source":["The default scoring is $R^2$ Each element in the array has the average $R^2$\u003c/sup\u003e value for the fold:\n"]},{"cell_type":"code","id":"665a4473-452b-4820-80e6-9e639e302be9","metadata":{},"outputs":[],"source":["R_cross"]},{"cell_type":"markdown","id":"3d46badf-d13c-4d98-b6b3-d96b9db51d56","metadata":{},"outputs":[],"source":["Since the model predicts data badly we get negative $R^2$ score. We can calculate the average and standard deviation of our estimate:\n"]},{"cell_type":"code","id":"9b26c457-f86c-4ef2-a5a3-7ef508559638","metadata":{},"outputs":[],"source":["print(\"The mean of the folds are\", R_cross.mean(), \"and the standard deviation is\", R_cross.std())"]},{"cell_type":"markdown","id":"7ed0b3d2-f136-4a5a-9f7e-d44732cdba9d","metadata":{},"outputs":[],"source":["\u003cdiv class=\"alert alert-danger alertdanger\" style=\"margin-top: 20px\"\u003e\n","\n","# **Question  #3):**\n","\n","**Calculate the average $R^2$ using two folds, then find the average $R^2$ for the second fold utilizing the \"AD\" feature:**\n","    \n","\u003c/div\u003e\n"]},{"cell_type":"code","id":"40d81aae-3a8d-477f-961e-dc1d44bb5027","metadata":{},"outputs":[],"source":["# Write your code below and press Shift+Enter to execute \n"]},{"cell_type":"markdown","id":"b35e3c4c-a4ab-4238-8e2a-bbfafa02f8c4","metadata":{},"outputs":[],"source":["\u003cdetails\u003e\u003csummary\u003eClick here for the solution\u003c/summary\u003e\n","\n","```python\n","R_cross_2 = cross_val_score(lre, x_data[[\"AD\"]], y_data, cv=2)\n","R_cross_2.mean()\n","\n","```\n","\n","\u003c/details\u003e\n"]},{"cell_type":"markdown","id":"f0f7a19f-049f-4070-9ab7-d027ff854511","metadata":{},"outputs":[],"source":["You can also use the function \u003ccode\u003ecross_val_predict\u003c/code\u003e to predict the output. The function splits up the data into the specified number of folds, with one fold for testing and the other folds are used for training. \n"]},{"cell_type":"markdown","id":"f1224463-f5ee-4d61-8add-3d13af8bc243","metadata":{},"outputs":[],"source":["We input the object, the feature \"AD\", and the target data y_data. The parameter \u003ccode\u003ecv\u003c/code\u003e determines the number of folds. In this case, it is 4. We can produce an output:\n"]},{"cell_type":"code","id":"44215c9b-027f-4ccd-854e-81e0efeeab08","metadata":{},"outputs":[],"source":["y_cvp = cross_val_predict(lre, x_data[[\"AD\"]], y_data, cv=4)\ny_cvp[0:5]"]},{"cell_type":"markdown","id":"f83c42de-b107-4640-a093-26bc935803d4","metadata":{},"outputs":[],"source":["Let's plot our predicted values and compare them to actual ones\n"]},{"cell_type":"code","id":"e2adc03c-3a0a-4963-bc66-553aac5e79d2","metadata":{},"outputs":[],"source":["plt.figure(figsize=(WIDTH, HEIGHT))\nplt.plot(df[[\"AD\"]], y_data, label=\"Avg_price\")\nplt.plot(df[[\"AD\"]], y_cvp, label=\"Avg_price predicted\")\nplt.title(\"Plot of actual Avg_price vs predicted by cv\")\nplt.xlabel(\"AD\")\nplt.ylabel(\"Price (in BUSD)\")\nplt.legend()"]},{"cell_type":"markdown","id":"b6765a65-d865-48d4-859d-efd2f0d56bdf","metadata":{},"outputs":[],"source":["How we can see our model predicts datapoints badly what we mentioned above\n"]},{"cell_type":"markdown","id":"419b0997-4417-4ca8-bf74-185fd0d8b711","metadata":{},"outputs":[],"source":["# **3. Overfitting, Underfitting and Model Selection**\n","\n","It turns out that the test data, sometimes referred to as the \"out of sample data\", is a much better measure of how well your model performs in the real world.  One reason for this is overfitting.\n","\n","Let's go over some examples. It turns out these differences are more apparent in Multiple Linear Regression and Polynomial Regression so we will explore overfitting in that context.\n"]},{"cell_type":"markdown","id":"3c2065d8-a282-423f-aed4-c97d1e54c27d","metadata":{},"outputs":[],"source":["Let's create Multiple Linear Regression objects and train the model using **\"ATR\"**, **\"OBV\"**, **\"RSI\"**, **\"AD\"** as features.\n"]},{"cell_type":"code","id":"05200220-1d0f-44c3-ade7-18297f65c298","metadata":{},"outputs":[],"source":["lr = LinearRegression()\nlr.fit(x_train[[\"ATR\", \"OBV\", \"RSI\", \"AD\"]], y_train)"]},{"cell_type":"markdown","id":"738ecd39-9956-4d96-bcc9-0980c54da626","metadata":{},"outputs":[],"source":["Prediction using training data:\n"]},{"cell_type":"code","id":"133d036d-74c8-4905-96e5-1d084e9bf207","metadata":{},"outputs":[],"source":["y_lr_train = lr.predict(x_train[[\"ATR\", \"OBV\", \"RSI\", \"AD\"]])\ny_lr_train[0:5]"]},{"cell_type":"markdown","id":"bfc2fc6f-a21c-4ffb-b6cf-5df194d155b4","metadata":{},"outputs":[],"source":["Prediction using test data:\n"]},{"cell_type":"code","id":"c88a62e3-bfc6-4e80-8a80-68f4fbfaade9","metadata":{},"outputs":[],"source":["y_lr_test = lr.predict(x_test[[\"ATR\", \"OBV\", \"RSI\", \"AD\"]])\ny_lr_test[0:5]"]},{"cell_type":"markdown","id":"2b019255-9044-4033-9ea5-ca0da6b3886d","metadata":{},"outputs":[],"source":["Let's perform some model evaluation using our training and testing data separately. Let's examine the distribution of the predicted values of the training data.\n"]},{"cell_type":"code","id":"0b9377b0-f5fe-4815-a766-3aa86ac1fb36","metadata":{},"outputs":[],"source":["title = \"Distribution plot of predicted values using training data\"\ndist_plot(y_train, y_lr_train, title)"]},{"cell_type":"markdown","id":"5dcb5778-f8b6-4f3a-b992-97491152f2df","metadata":{},"outputs":[],"source":["**Figure 1:** Plot of predicted values using the training data compared to the actual values of the training data.\n"]},{"cell_type":"markdown","id":"4d94e5ca-399b-4011-a2b4-b690363dce23","metadata":{},"outputs":[],"source":["So far, the model seems to be doing well in learning from the training dataset. But what happens when the model encounters new data from the testing dataset? When the model generates new values from the test data, we see the distribution of the predicted values is much different from the actual target values.\n"]},{"cell_type":"code","id":"798db5e4-e928-4936-8ce7-2aed66d35b14","metadata":{},"outputs":[],"source":["title = \"Distribution plot of predicted values using test data\"\ndist_plot(y_test, y_lr_test, title)"]},{"cell_type":"markdown","id":"b1f70bd5-33f8-420a-9b7f-c18cf07d2ceb","metadata":{},"outputs":[],"source":["**Figure 2:** Plot of predicted value using the test data compared to the actual values of the test data.\n"]},{"cell_type":"markdown","id":"2c63e790-c0d4-4aa8-ac37-657206933060","metadata":{},"outputs":[],"source":["Comparing **Figure 1** and **Figure 2**, we can see that values in **Figure 1** are much closer than in **Figure 2**. This difference in **Figure 2** is apparent in the range of $(0.76; 0.81)$ and $(0.82; 0.85)$. This is where the shape of the distribution is a bit different. Let's see if polynomial regression also exhibits a drop in the prediction accuracy when analysing the test dataset.\n"]},{"cell_type":"markdown","id":"c1b88f6b-12d0-4d7b-8e35-6d48541d8ce8","metadata":{},"outputs":[],"source":["## **Overfitting**\n","\n","\u003ccenter\u003e\n","    \u003cimg src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMSkillsNetwork-GPXX08XFEN/6360ef2568a0381c60b26049_overfitting-and-underfitting-in-machine-learning-1.png\" alt=\"overfitting\"\u003e\n","\u003c/center\u003e\n","\n","\u003ccenter\u003eThe example of overfitting\u003c/center\u003e\n"]},{"cell_type":"markdown","id":"be9fcf9f-0cbf-4bcb-8f2c-8bff310480b3","metadata":{},"outputs":[],"source":["**Overfitting** is an undesirable machine learning behavior that occurs when the machine learning model gives accurate predictions for training data but not for new data. When data scientists use machine learning models for making predictions, they first train the model on a known data set. Then, based on this information, the model tries to predict outcomes for new data sets. An overfit model can give inaccurate predictions and cannot perform well for all types of new data.\n","\n","**Overfitting** occurs when the model fits the noise, but not the underlying process. Therefore, when testing your model using the test set, your model does not perform as well since it is modelling noise, not the underlying process that generated the relationship. Let's create a degree 5 polynomial model.\n"]},{"cell_type":"markdown","id":"6058ff86-2bf0-4637-a57d-e613036a4906","metadata":{},"outputs":[],"source":["Let's use 55 percent of the data for training and the rest for testing:\n"]},{"cell_type":"code","id":"407e58eb-ea70-44b2-8937-f181f8dd71c0","metadata":{},"outputs":[],"source":["x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, test_size=0.45, shuffle=False)"]},{"cell_type":"markdown","id":"21b6714e-d677-4e15-80ef-52a9711d9d5b","metadata":{},"outputs":[],"source":["We will perform a degree 5 polynomial transformation on the feature \"AD\".\n"]},{"cell_type":"code","id":"d489e57c-ed66-4a7c-8a12-3ff60c371512","metadata":{},"outputs":[],"source":["pr = PolynomialFeatures(degree=5)\nx_train_pr = pr.fit_transform(x_train[[\"AD\"]])\nx_test_pr = pr.fit_transform(x_test[[\"AD\"]])\npr"]},{"cell_type":"markdown","id":"1ed5d441-ec9f-4dd6-ae1e-ae5e9050a18b","metadata":{},"outputs":[],"source":["Now, let's create a Linear Regression model `poly` and train it.\n"]},{"cell_type":"code","id":"5ed9e5a1-e446-4ffd-ba9d-c1e5fa7b5ae4","metadata":{},"outputs":[],"source":["poly = LinearRegression()\npoly.fit(x_train_pr, y_train)"]},{"cell_type":"markdown","id":"83b3b11e-6c55-4378-822f-566be8c0cce0","metadata":{},"outputs":[],"source":["We can see the output of our model using the method \u003ccode\u003epredict\u003c/code\u003e. We assign the values to `y_hat`.\n"]},{"cell_type":"code","id":"393f660e-06e8-4299-a533-e8207594ef5b","metadata":{},"outputs":[],"source":["y_poly = poly.predict(x_test_pr)\ny_poly[0:5]"]},{"cell_type":"markdown","id":"ebd928b1-d969-4355-bf2e-33307062bf8e","metadata":{},"outputs":[],"source":["Let's take the first five predicted values and compare it to the actual targets.\n"]},{"cell_type":"code","id":"65b25832-002f-4b6e-a813-e9d298340e7b","metadata":{},"outputs":[],"source":["print(\"Predicted values:\", y_poly[0:4])\nprint(\"True values:\", y_test[0:4].values)"]},{"cell_type":"markdown","id":"8f1f17a9-29c9-4a0e-8d2f-4b7b8a2ee823","metadata":{},"outputs":[],"source":["We will use the function \u003ccode\u003epoly_plot\u003c/code\u003e that we defined at the beginning of the lab to display the training data, testing data, and the predicted function.\n"]},{"cell_type":"code","id":"b4481eac-451a-4050-98f6-49c4aa223b4d","metadata":{},"outputs":[],"source":["poly_plot(x_train[[\"AD\"]], x_test[[\"AD\"]], y_train, y_test, poly, pr, \"AD\")"]},{"cell_type":"markdown","id":"0cdd4f3b-2285-414c-b8a6-159817b616c1","metadata":{},"outputs":[],"source":["**Figure 3:** A polynomial regression model where red dots represent training data, green dots represent test data, and the blue line represents the model prediction.\n"]},{"cell_type":"markdown","id":"b9be1f91-64db-4dd0-9e60-6e378b486dcf","metadata":{},"outputs":[],"source":["Let's calculate $R^2$ for train and test dataset\n"]},{"cell_type":"markdown","id":"cce67f67-bb41-4a5d-9f43-425a0297926f","metadata":{},"outputs":[],"source":["$R^2$ of the training data:\n"]},{"cell_type":"code","id":"262bebb6-f691-48be-9a7d-41fe9e34934b","metadata":{},"outputs":[],"source":["poly.score(x_train_pr, y_train)"]},{"cell_type":"markdown","id":"c479e291-c8a3-4397-999f-1fd804a4e8b6","metadata":{},"outputs":[],"source":["$R^2$ of the test data:\n"]},{"cell_type":"code","id":"9b9e55dc-00d5-468d-833c-48e19e480e01","metadata":{},"outputs":[],"source":["poly.score(x_test_pr, y_test)"]},{"cell_type":"markdown","id":"279d4fe5-5c5a-47cc-99ee-178a80e8e60c","metadata":{},"outputs":[],"source":["We got negative $R^2$ score calculated from test dataset again. That means that our model predicts unseen datapoints not good\n"]},{"cell_type":"markdown","id":"e73854ea-ab23-42e2-b07e-747add4b589c","metadata":{},"outputs":[],"source":["Let's see how the $MSE$ changes on the test data for different order polynomials and then plot the results:\n"]},{"cell_type":"code","id":"cc81462f-1d01-4574-adc4-e166d6999021","metadata":{},"outputs":[],"source":["mse_test = []\n\norder = [1, 2, 3, 4]\nfor n in order:\n    pr = PolynomialFeatures(degree=n)\n    x_train_pr = pr.fit_transform(x_train[[\"AD\"]])\n    x_test_pr = pr.fit_transform(x_test[[\"AD\"]])\n    \n    lrt = LinearRegression()\n    lrt.fit(x_train_pr, y_train)\n    \n    mse = mean_squared_error(y_test, lrt.predict(x_test_pr) )\n    mse_test.append(mse)\n\nplt.figure(figsize=(WIDTH, HEIGHT))\nplt.plot(order, mse_test)\nplt.xlabel(\"Order\")\nplt.ylabel(\"MSE\")\nplt.title(\"MSE using test data\")"]},{"cell_type":"markdown","id":"9bc71206-3685-4ed7-8478-de31de6aa693","metadata":{},"outputs":[],"source":["We see with increasing order of polynomial $MSE$ increases as well\n"]},{"cell_type":"markdown","id":"82ae9528-2907-414a-9a96-8a444eb252a3","metadata":{},"outputs":[],"source":["The following function will be used in the next section. Please run the cell below.\n"]},{"cell_type":"code","id":"766ae29f-db1e-440b-a75f-5b4f472fe171","metadata":{},"outputs":[],"source":["def f(order: tuple, test_size: tuple, indicator: str) -\u003e None:\n    \"\"\"\n    Plots train, test data and predicted function with test_size = `test_size` and order = `order`\n    \n    Parameters\n    ----------\n    order: tuple\n        Tuple of orders\n    test_size: tuple\n        Tuple of test sizes\n    indicator: str\n        The indicator\n    \"\"\"\n    x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, test_size=test_size, shuffle=False)\n    pr = PolynomialFeatures(degree=order)\n    x_train_pr = pr.fit_transform(x_train[[\"AD\"]])\n    x_test_pr = pr.fit_transform(x_test[[\"AD\"]])\n    \n    poly = LinearRegression()\n    poly.fit(x_train_pr, y_train)\n    \n    poly_plot(x_train[[\"AD\"]], x_test[[\"AD\"]], y_train, y_test, poly, pr, indicator)\n    plt.show()"]},{"cell_type":"markdown","id":"91b81ce2-69eb-4754-a2f2-0534e8297eea","metadata":{},"outputs":[],"source":["The following interface allows you to experiment with different polynomial orders and different amounts of data.\n"]},{"cell_type":"code","id":"e8e0b0b5-2fe8-4fd2-b27d-34f7e22313bd","metadata":{},"outputs":[],"source":["interact(f, order=(0, 6, 1), test_size=(0.05, 0.95, 0.05), indicator=\"AD\")"]},{"cell_type":"markdown","id":"d17729e1-1c17-4b5a-8e45-0868105fa359","metadata":{},"outputs":[],"source":["We can move this slider and change the order and we will see how our predicted function changes. As we increase order out function gets complicated. As we increase \u003ccode\u003etest_size\u003c/code\u003e number of red increases\n"]},{"cell_type":"markdown","id":"75e7ca89-a5a3-471a-9da7-1838785cda24","metadata":{},"outputs":[],"source":["\u003cdiv class=\"alert alert-danger alertdanger\" style=\"margin-top: 20px\"\u003e\n","    \n","# **Question  #4 a):**\n","\n","**We can perform polynomial transformations with more than one feature. Create a `PolynomialFeatures` object `pr1` of degree two.**\n","\n","\u003c/div\u003e\n"]},{"cell_type":"code","id":"bad02a4a-7485-4cda-92ad-abb4996f3642","metadata":{},"outputs":[],"source":["# Write your code below and press Shift+Enter to execute \n"]},{"cell_type":"markdown","id":"7699f707-b780-40f5-83f6-ab91e7c37109","metadata":{},"outputs":[],"source":["\u003cdetails\u003e\u003csummary\u003eClick here for the solution\u003c/summary\u003e\n","\n","```python\n","pr1 = PolynomialFeatures(degree=2)\n","\n","```\n","\n","\u003c/details\u003e\n"]},{"cell_type":"markdown","id":"8622e1af-ff62-4ec4-955a-6596233d32d8","metadata":{},"outputs":[],"source":["\u003cdiv class=\"alert alert-danger alertdanger\" style=\"margin-top: 20px\"\u003e\n","    \n","# **Question  #4 b):**\n","\n","**Transform the training and testing samples for the features \"ATR\", \"OBV\", \"RSI\", \"AD\". Hint: use the method `fit_transform`**\n","\u003c/div\u003e\n"]},{"cell_type":"code","id":"fc7cbf35-b0e8-4467-b042-b011f393e5a0","metadata":{},"outputs":[],"source":["# Write your code below and press Shift+Enter to execute \n"]},{"cell_type":"markdown","id":"b30bea37-886b-4593-93f2-a4f94f8336c2","metadata":{},"outputs":[],"source":["\u003cdetails\u003e\u003csummary\u003eClick here for the solution\u003c/summary\u003e\n","\n","```python\n","x_train_pr1 = pr1.fit_transform(x_train[[\"ATR\", \"OBV\", \"RSI\", \"AD\"]])\n","x_test_pr1 = pr1.fit_transform(x_test[[\"ATR\", \"OBV\", \"RSI\", \"AD\"]])\n","\n","\n","```\n","\n","\u003c/details\u003e\n"]},{"cell_type":"markdown","id":"6f763c87-7395-4f81-8b2b-ea643df9f733","metadata":{},"outputs":[],"source":["\u003c!-- The answer is below:\n","\n","x_train_pr1=pr.fit_transform(x_train[['horsepower', 'curb-weight', 'engine-size', 'highway-mpg']])\n","x_test_pr1=pr.fit_transform(x_test[['horsepower', 'curb-weight', 'engine-size', 'highway-mpg']])\n","\n","--\u003e\n"]},{"cell_type":"markdown","id":"93cfd80a-ca1c-458a-b2b4-ddb043e92c45","metadata":{},"outputs":[],"source":["\u003cdiv class=\"alert alert-danger alertdanger\" style=\"margin-top: 20px\"\u003e\n","    \n","# **Question  #4 c):**\n","\n","**How many dimensions does the new feature have?**\n","\n","**Hint: use the attribute `.shape`.**\n","    \n","\u003c/div\u003e\n"]},{"cell_type":"code","id":"32db99cb-cb83-4cf0-b4c0-7ee00d3336a9","metadata":{},"outputs":[],"source":["# Write your code below and press Shift+Enter to execute \n"]},{"cell_type":"markdown","id":"f0345901-a145-4e12-a26d-0895112fdf8e","metadata":{},"outputs":[],"source":["\u003cdetails\u003e\u003csummary\u003eClick here for the solution\u003c/summary\u003e\n","\n","```python\n","# there are now 15 features\n","x_train_pr1.shape\n","```\n","\n","\u003c/details\u003e\n"]},{"cell_type":"markdown","id":"183c0df2-7028-4642-bafe-40756c3e7b21","metadata":{},"outputs":[],"source":["\u003cdiv class=\"alert alert-danger alertdanger\" style=\"margin-top: 20px\"\u003e\n","    \n","# **Question  #4 d):**\n","\n","**Create a linear regression model `poly1`. Train the object using the method `fit` using the polynomial features.**\n","    \n","\u003c/div\u003e\n"]},{"cell_type":"code","id":"e35519ad-2fc6-417c-96f2-38a880f67bf3","metadata":{},"outputs":[],"source":["# Write your code below and press Shift+Enter to execute \n"]},{"cell_type":"markdown","id":"6b6f65f4-613c-4c1e-a20a-45949a6913b3","metadata":{},"outputs":[],"source":["\u003cdetails\u003e\u003csummary\u003eClick here for the solution\u003c/summary\u003e\n","\n","```python\n","poly1 = LinearRegression()\n","poly1.fit(x_train_pr1, y_train)\n","```\n","\n","\u003c/details\u003e\n"]},{"cell_type":"markdown","id":"7742d6a3-4ca3-4756-9123-aceb76327f19","metadata":{},"outputs":[],"source":["\u003cdiv class=\"alert alert-danger alertdanger\" style=\"margin-top: 20px\"\u003e\n","    \n","# **Question  #4 e):**\n","    \n","**Use the method  `predict` to predict an output on the polynomial features, then use the function `dist_plot` to display the distribution of the predicted test output vs the actual test data.**\n","    \n","\u003c/div\u003e\n"]},{"cell_type":"code","id":"80d31337-db6b-4b47-8d74-a1c1508eb6e2","metadata":{},"outputs":[],"source":["# Write your code below and press Shift+Enter to execute \n"]},{"cell_type":"markdown","id":"bd3d5f34-dafc-4f2e-b3bb-5f9147ec7527","metadata":{},"outputs":[],"source":["\u003cdetails\u003e\u003csummary\u003eClick here for the solution\u003c/summary\u003e\n","\n","```python\n","y_poly1 = poly1.predict(x_test_pr1)\n","\n","title = \"Distribution plot of predicted values using test data\"\n","\n","dist_plot(y_test, y_poly1, title)\n","\n","```\n","\n","\u003c/details\u003e\n"]},{"cell_type":"markdown","id":"3a9d7c3c-3564-47dc-962f-9fd12e3ae453","metadata":{},"outputs":[],"source":["\u003cdiv class=\"alert alert-danger alertdanger\" style=\"margin-top: 20px\"\u003e\n","\n","# **Question  #4 f):**\n","\n","**Using the distribution plot above, describe (in words) the two regions where the predicted prices are less accurate than the actual prices.**\n","\n","\u003c/div\u003e\n"]},{"cell_type":"code","id":"e87a3410-7afc-4343-af54-f66425d1ccc7","metadata":{},"outputs":[],"source":["# Write your code below and press Shift+Enter to execute \n"]},{"cell_type":"markdown","id":"b2d34a42-de2b-4b69-af33-e88ef719e11d","metadata":{},"outputs":[],"source":["\u003cdetails\u003e\u003csummary\u003eClick here for the solution\u003c/summary\u003e\n","\n","```python\n","# 1. 0.75 - 0.81 We see how `y_pred` is much lower than `y`\n","# 2. 0.81 - 0.89 `y_pred` increased too much\n","```\n","\n","\u003c/details\u003e\n"]},{"cell_type":"markdown","id":"4f48f7b7-9b22-4853-ab0d-b83f2126d1b5","metadata":{},"outputs":[],"source":["# **4. Ridge Regression**\n"]},{"cell_type":"markdown","id":"0884e3ed-7955-411d-be26-c220f3ef81be","metadata":{},"outputs":[],"source":["**Ridge regression** is a method of estimating the coefficients of multiple-regression models in scenarios where the independent variables are highly correlated. It has been used in many fields including econometrics, chemistry, and engineering. Also known as Tikhonov regularization, named for Andrey Tikhonov, it is a method of regularization of ill-posed problems. It is particularly useful to mitigate the problem of multicollinearity in linear regression, which commonly occurs in models with large numbers of parameters. In general, the method provides improved efficiency in parameter estimation problems in exchange for a tolerable amount of bias (see bias–variance tradeoff).\n","\n","\n","In this section, we will review Ridge Regression and see how the parameter alpha changes the model. Just a note, here our test data will be used as validation data.\n"]},{"cell_type":"markdown","id":"599da8a2-1d27-42cc-b255-8a430e8067f6","metadata":{},"outputs":[],"source":["Let's perform a degree two polynomial transformation on our data.\n"]},{"cell_type":"code","id":"c151f6bb-0e5d-4d20-93d3-77f2b60d7598","metadata":{},"outputs":[],"source":["pr = PolynomialFeatures(degree=2)\nx_train_pr = pr.fit_transform(x_train[[\"ATR\", \"OBV\", \"RSI\", \"AD\"]])\nx_test_pr = pr.fit_transform(x_test[[\"ATR\", \"OBV\", \"RSI\", \"AD\"]])"]},{"cell_type":"markdown","id":"22a077f1-425c-4820-b009-9882843c062a","metadata":{},"outputs":[],"source":["Let's create a \u003ccode\u003eRidge\u003c/code\u003e regression object, setting the regularization parameter (alpha) to 1\n"]},{"cell_type":"code","id":"92e1d88f-c664-40da-b6bd-e33c7f365b6a","metadata":{},"outputs":[],"source":["rm = Ridge(alpha=1)"]},{"cell_type":"markdown","id":"1fdd8e73-cf96-4c80-b5a9-2208963da29f","metadata":{},"outputs":[],"source":["Like regular regression, you can fit the model using the method \u003ccode\u003efit\u003c/code\u003e\n"]},{"cell_type":"code","id":"91f0a63c-d325-452a-b8d9-33618fb56b6e","metadata":{},"outputs":[],"source":["rm.fit(x_train_pr, y_train)"]},{"cell_type":"markdown","id":"37c8fdc0-7f2b-490f-89f6-81c6166360e7","metadata":{},"outputs":[],"source":["Similarly, you can obtain a prediction:\n"]},{"cell_type":"code","id":"9c90b6ee-2c7f-4e26-96aa-50e8d39049b0","metadata":{},"outputs":[],"source":["y_pr = rm.predict(x_test_pr)"]},{"cell_type":"markdown","id":"65a82f4b-2016-49ba-89b9-c40dabaa771a","metadata":{},"outputs":[],"source":["Let's compare the first five predicted samples to our test set:\n"]},{"cell_type":"code","id":"6ca23a8b-4076-4c15-a5a2-f52ffc074b85","metadata":{},"outputs":[],"source":["print(\"Predicted:\", y_pr[0:4])\nprint(\"Test set :\", y_test[0:4].values)"]},{"cell_type":"markdown","id":"00c53f25-6fbb-48f5-a843-b7038403d302","metadata":{},"outputs":[],"source":["We select the value of alpha that minimizes the test error. To do so, we can use a for loop. We have also created a progress bar to see how many iterations we have completed so far.\n"]},{"cell_type":"code","id":"9f924d06-1e09-4bb1-a77a-169d444a83ee","metadata":{},"outputs":[],"source":["R_squared_test = []\nR_squared_train = []\n\nalphas = np.array(range(0, 5))\npbar = tqdm(alphas)\n\nfor alpha in pbar:\n    ridge_model = Ridge(alpha=alpha) \n    ridge_model.fit(x_train_pr, y_train)\n    test_score, train_score = ridge_model.score(x_test_pr, y_test), ridge_model.score(x_train_pr, y_train)\n    \n    pbar.set_postfix({\"Test Score\": test_score, \"Train Score\": train_score})\n\n    R_squared_test.append(test_score)\n    R_squared_train.append(train_score)"]},{"cell_type":"markdown","id":"bde3e424-a92f-40ff-9f7b-19d6704dc173","metadata":{},"outputs":[],"source":["We can plot out the value of $R^2$ for different alphas:\n"]},{"cell_type":"code","id":"ff76bd85-4e7e-44a4-ab17-5fbcc6b5578b","metadata":{},"outputs":[],"source":["plt.figure(figsize=(WIDTH, HEIGHT))\nplt.plot(alphas, R_squared_test, label=\"validation data\")\nplt.plot(alphas, R_squared_train, \"r\", label=\"training Data\")\nplt.title(\"Plot of Alpha vs R-squared\")\nplt.xlabel(\"Alpha\")\nplt.ylabel(\"R-squared\")\nplt.legend()"]},{"cell_type":"markdown","id":"eaa7e80b-db08-4ad6-8dfa-da3b6f2e9a70","metadata":{},"outputs":[],"source":["**Figure 4:** The blue line represents the $R^2$ of the validation data, and the red line represents the $R^2$ of the training data. We see an improvement when order gets 1 but $R^2$ remains the same after that. The x-axis represents the different values of alphas.\n"]},{"cell_type":"markdown","id":"0f117a1d-2b6f-414a-bf8b-9e5a9afb52f9","metadata":{},"outputs":[],"source":["Here the model is built and tested on the same data, so the training and test data are the same.\n","\n","The red line in **Figure 4** represents the $R^2$ of the training data. $R^2$ remains the same over alpha from 0 to 4\n"]},{"cell_type":"markdown","id":"ff89e15c-49e8-417a-b7a4-5b4f5b4b76e8","metadata":{},"outputs":[],"source":["\u003cdiv class=\"alert alert-danger alertdanger\" style=\"margin-top: 20px\"\u003e\n","\n","# **Question  #5):**\n","\n","**Perform Ridge regression. Calculate the $R^2$ using the polynomial features, use the training data to train the model and use the test data to test the model. The parameter alpha should be set to 10.**\n","\n","\u003c/div\u003e\n"]},{"cell_type":"code","id":"4d8407a6-bac2-4d52-af33-9279ba50497b","metadata":{},"outputs":[],"source":["# Write your code below and press Shift+Enter to execute \n"]},{"cell_type":"markdown","id":"f00453ed-5dcf-4647-9555-7486aaeb53e2","metadata":{},"outputs":[],"source":["\u003cdetails\u003e\u003csummary\u003eClick here for the solution\u003c/summary\u003e\n","\n","```python\n","ridge_model2 = Ridge(alpha=10) \n","ridge_model2.fit(x_train_pr, y_train)\n","ridge_model2.score(x_test_pr, y_test)\n","\n","```\n","\n","\u003c/details\u003e\n"]},{"cell_type":"markdown","id":"6adef89a-d6fc-4d07-8701-1d4df97a3880","metadata":{},"outputs":[],"source":["# **5. Grid Search**\n","\n","\u003ccenter\u003e\n","    \u003cimg src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMSkillsNetwork-GPXX08XFEN/An-introduction-to-GridSearchCV-and-RandomizedSearchCV-Image.webp\" alt=\"grid-search\"\u003e\n","\u003c/center\u003e\n","\n","\u003ccenter\u003eThe example of how Grid Search works\u003c/center\u003e\n"]},{"cell_type":"markdown","id":"f67e78e8-c87f-4102-b6f8-f685835b4470","metadata":{},"outputs":[],"source":["`GridSearchCV` is the process of performing hyperparameter tuning in order to determine the optimal values for a given model. The performance of a model significantly depends on the value of hyperparameters. Note that there is no way to know in advance the best values for hyperparameters so ideally, we need to try all possible values to know the optimal values. Doing this manually could take a considerable amount of time and resources and thus we use GridSearchCV to automate the tuning of hyperparameters.\n"]},{"cell_type":"markdown","id":"b7ffe731-09f2-4152-98ba-c19df30e25a8","metadata":{},"outputs":[],"source":["The term alpha is a hyperparameter. Sklearn has the class \u003ccode\u003eGridSearchCV\u003c/code\u003e to make the process of finding the best hyperparameter simpler.\n"]},{"cell_type":"markdown","id":"b4dd156c-332a-44c1-8d80-4e19c8131dd5","metadata":{},"outputs":[],"source":["We create a dictionary of parameter values:\n"]},{"cell_type":"code","id":"c0c0b125-8cf5-4a52-b313-c33f9cfaf2df","metadata":{},"outputs":[],"source":["parameters = [{\"alpha\": [0.01, 0.1, 1, 10]}]\nparameters"]},{"cell_type":"markdown","id":"b7d83f3b-a782-4d42-807b-868beb5381e2","metadata":{},"outputs":[],"source":["Create a \u003ccode\u003eRidge\u003c/code\u003e regression object:\n"]},{"cell_type":"code","id":"f8bd64d6-9b36-4c5d-982e-bc8cb41b8aee","metadata":{},"outputs":[],"source":["rr = Ridge()\nrr"]},{"cell_type":"markdown","id":"ee270774-3192-4c13-8539-b0b406a77d17","metadata":{},"outputs":[],"source":["Create a ridge grid search object:\n"]},{"cell_type":"code","id":"4a17a152-2297-4b27-89e8-40ee91cc487c","metadata":{},"outputs":[],"source":["grid = GridSearchCV(rr, parameters, cv=4)"]},{"cell_type":"markdown","id":"f51fb5d8-1667-413a-abbf-7bdda322b043","metadata":{},"outputs":[],"source":["Fit the model:\n"]},{"cell_type":"code","id":"dd80021f-39a1-48a2-8e46-f3429ef4bc64","metadata":{},"outputs":[],"source":["grid.fit(x_data[[\"ATR\", \"OBV\", \"RSI\", \"AD\"]], y_data)"]},{"cell_type":"markdown","id":"5a63dd89-fea6-485a-a69f-8686ac7cb0de","metadata":{},"outputs":[],"source":["The object finds the best parameter values on the validation data. We can obtain the estimator with the best parameters and assign it to the variable `best_RR` as follows:\n"]},{"cell_type":"code","id":"63a3a769-3892-4f90-acf1-2870d7b365e2","metadata":{},"outputs":[],"source":["best_rr = grid.best_estimator_\nbest_rr"]},{"cell_type":"markdown","id":"03e16068-ba17-425d-9315-66411effd01a","metadata":{},"outputs":[],"source":["We now test our model on the test data:\n"]},{"cell_type":"code","id":"af790b68-fc66-40ab-966c-ea05d849ee98","metadata":{},"outputs":[],"source":["best_rr.score(x_test[[\"ATR\", \"OBV\", \"RSI\", \"AD\"]], y_test)"]},{"cell_type":"markdown","id":"20ce3444-d6e5-41bf-b8e8-fe8bdf3d0fb8","metadata":{},"outputs":[],"source":["Let's see predicted values by the best model and compare them to actual ones\n"]},{"cell_type":"code","id":"d8753a94-7326-43dc-9156-718298ce320f","metadata":{},"outputs":[],"source":["y_rr = best_rr.predict(x_test[[\"ATR\", \"OBV\", \"RSI\", \"AD\"]])\ndist_plot(y_test, y_rr, \"Plot of distributions of `y_test` and `y_rr`\")"]},{"cell_type":"markdown","id":"bf978113-f6e5-4da7-be0b-3aea4f18ba9e","metadata":{},"outputs":[],"source":["Despite the high $R^2$ score, the distributions are not very close compared to multiple linear regression\n"]},{"cell_type":"markdown","id":"39b6e290-0702-4aa2-908c-8a548d304e7f","metadata":{},"outputs":[],"source":["# **Conclusion:**\n","\n","We learned how to split our dataset into train and test datasets. We found out what **overfitting**, **Ridge regression**, **Grid Search**, **Cross-validation** are and learned how to use these model and we got the best model with $R^2$ of 0.762\n"]},{"cell_type":"markdown","id":"1485fd50-a2d3-4156-a125-beb33e0d1aed","metadata":{},"outputs":[],"source":["# **6. Sources**\n","\n","\u003cul\u003e\n","    \u003cli\u003e\u003ca target=\"_blank\" href=\"https://en.wikipedia.org/wiki/Cross-validation_(statistics)?utm_medium=Exinfluencer\u0026utm_source=Exinfluencer\u0026utm_content=000026UJ\u0026utm_term=10006555\u0026utm_id=NA-SkillsNetwork-Channel-SkillsNetworkGuidedProjectsIBMSkillsNetworkGPXX08XFEN2550-2023-01-01\"\u003ehttps://en.wikipedia.org/wiki/Cross-validation_(statistics)\u003c/a\u003e\u003c/li\u003e\n","    \u003cli\u003e\u003ca target=\"_blank\" href=\"https://aws.amazon.com/what-is/overfitting/?utm_medium=Exinfluencer\u0026utm_source=Exinfluencer\u0026utm_content=000026UJ\u0026utm_term=10006555\u0026utm_id=NA-SkillsNetwork-Channel-SkillsNetworkGuidedProjectsIBMSkillsNetworkGPXX08XFEN2550-2023-01-01\"\u003ehttps://aws.amazon.com/what-is/overfitting/\u003c/a\u003e\u003c/li\u003e\n","    \u003cli\u003e\u003ca target=\"_blank\" href=\"https://en.wikipedia.org/wiki/Ridge_regression?utm_medium=Exinfluencer\u0026utm_source=Exinfluencer\u0026utm_content=000026UJ\u0026utm_term=10006555\u0026utm_id=NA-SkillsNetwork-Channel-SkillsNetworkGuidedProjectsIBMSkillsNetworkGPXX08XFEN2550-2023-01-01\"\u003ehttps://en.wikipedia.org/wiki/Ridge_regression\u003c/a\u003e\u003c/li\u003e\n","    \u003cli\u003e\u003ca target=\"_blank\" href=\"https://www.mygreatlearning.com/blog/gridsearchcv/?utm_medium=Exinfluencer\u0026utm_source=Exinfluencer\u0026utm_content=000026UJ\u0026utm_term=10006555\u0026utm_id=NA-SkillsNetwork-Channel-SkillsNetworkGuidedProjectsIBMSkillsNetworkGPXX08XFEN2550-2023-01-01\"\u003ehttps://www.mygreatlearning.com/blog/gridsearchcv/\u003c/a\u003e\u003c/li\u003e\n","    \u003cli\u003e\u003ca target=\"_blank\" href=\"https://developers.google.com/machine-learning/crash-course/training-and-test-sets/splitting-data?utm_medium=Exinfluencer\u0026utm_source=Exinfluencer\u0026utm_content=000026UJ\u0026utm_term=10006555\u0026utm_id=NA-SkillsNetwork-Channel-SkillsNetworkGuidedProjectsIBMSkillsNetworkGPXX08XFEN2550-2023-01-01\"\u003ehttps://developers.google.com/machine-learning/crash-course/training-and-test-sets/splitting-data\u003c/a\u003e\u003c/li\u003e\n","    \u003cli\u003e\u003ca target=\"_blank\" href=\"https://developers.google.com/static/machine-learning/crash-course/images/PartitionTwoSets.svg?utm_medium=Exinfluencer\u0026utm_source=Exinfluencer\u0026utm_content=000026UJ\u0026utm_term=10006555\u0026utm_id=NA-SkillsNetwork-Channel-SkillsNetworkGuidedProjectsIBMSkillsNetworkGPXX08XFEN2550-2023-01-01\"\u003ehttps://developers.google.com/static/machine-learning/crash-course/images/PartitionTwoSets.svg\u003c/a\u003e\u003c/li\u003e\n","    \u003cli\u003e\u003ca target=\"_blank\" href=\"https://es.mathworks.com/discovery/cross-validation.html?utm_medium=Exinfluencer\u0026utm_source=Exinfluencer\u0026utm_content=000026UJ\u0026utm_term=10006555\u0026utm_id=NA-SkillsNetwork-Channel-SkillsNetworkGuidedProjectsIBMSkillsNetworkGPXX08XFEN2550-2023-01-01\"\u003ehttps://es.mathworks.com/discovery/cross-validation.html\u003c/a\u003e\u003c/li\u003e\n","    \u003cli\u003e\u003ca target=\"_blank\" href=\"https://es.mathworks.com/discovery/cross-validation/_jcr_content/mainParsys/image.adapt.full.medium.jpg/1676051591395.jpg?utm_medium=Exinfluencer\u0026utm_source=Exinfluencer\u0026utm_content=000026UJ\u0026utm_term=10006555\u0026utm_id=NA-SkillsNetwork-Channel-SkillsNetworkGuidedProjectsIBMSkillsNetworkGPXX08XFEN2550-2023-01-01\"\u003ehttps://es.mathworks.com/discovery/cross-validation/_jcr_content/mainParsys/image.adapt.full.medium.jpg/1676051591395.jpg\u003c/a\u003e\u003c/li\u003e\n","    \u003cli\u003e\u003ca target=\"_blank\" href=\"https://www.superannotate.com/blog/overfitting-and-underfitting-in-machine-learning?utm_medium=Exinfluencer\u0026utm_source=Exinfluencer\u0026utm_content=000026UJ\u0026utm_term=10006555\u0026utm_id=NA-SkillsNetwork-Channel-SkillsNetworkGuidedProjectsIBMSkillsNetworkGPXX08XFEN2550-2023-01-01\"\u003ehttps://www.superannotate.com/blog/overfitting-and-underfitting-in-machine-learning\u003c/a\u003e\u003c/li\u003e\n","    \u003cli\u003e\u003ca target=\"_blank\" href=\"https://uploads-ssl.webflow.com/614c82ed388d53640613982e/6360ef2568a0381c60b26049_overfitting-and-underfitting-in-machine-learning-1.png?utm_medium=Exinfluencer\u0026utm_source=Exinfluencer\u0026utm_content=000026UJ\u0026utm_term=10006555\u0026utm_id=NA-SkillsNetwork-Channel-SkillsNetworkGuidedProjectsIBMSkillsNetworkGPXX08XFEN2550-2023-01-01\"\u003ehttps://uploads-ssl.webflow.com/614c82ed388d53640613982e/6360ef2568a0381c60b26049_overfitting-and-underfitting-in-machine-learning-1.png\u003c/a\u003e\u003c/li\u003e\n","    \u003cli\u003e\u003ca target=\"_blank\" href=\"https://sqlrelease.com/an-introduction-to-gridsearchcv-and-randomizedsearchcv?utm_medium=Exinfluencer\u0026utm_source=Exinfluencer\u0026utm_content=000026UJ\u0026utm_term=10006555\u0026utm_id=NA-SkillsNetwork-Channel-SkillsNetworkGuidedProjectsIBMSkillsNetworkGPXX08XFEN2550-2023-01-01\"\u003ehttps://sqlrelease.com/an-introduction-to-gridsearchcv-and-randomizedsearchcv\u003c/a\u003e\u003c/li\u003e\n","    \u003cli\u003e\u003ca target=\"_blank\" href=\"https://i0.wp.com/sqlrelease.com/wp-content/uploads/2021/08/An-introduction-to-GridSearchCV-and-RandomizedSearchCV-Image.jpg?utm_medium=Exinfluencer\u0026utm_source=Exinfluencer\u0026utm_content=000026UJ\u0026utm_term=10006555\u0026utm_id=NA-SkillsNetwork-Channel-SkillsNetworkGuidedProjectsIBMSkillsNetworkGPXX08XFEN2550-2023-01-01\u0026ssl=1\"\u003ehttps://i0.wp.com/sqlrelease.com/wp-content/uploads/2021/08/An-introduction-to-GridSearchCV-and-RandomizedSearchCV-Image.jpg?ssl=1\u003c/a\u003e\u003c/li\u003e\n","\u003c/ul\u003e\n"]},{"cell_type":"markdown","id":"251a74d2-a349-430c-b7a1-0c7c2f682652","metadata":{},"outputs":[],"source":["# **Thank you for completing this lab!**\n","\n","## Author\n","\n","\u003ca href=\"https://author.skills.network/instructors/borys_melnychuk?utm_medium=Exinfluencer\u0026utm_source=Exinfluencer\u0026utm_content=000026UJ\u0026utm_term=10006555\u0026utm_id=NA-SkillsNetwork-Channel-SkillsNetworkGuidedProjectsIBMSkillsNetworkGPXX08XFEN2550-2023-01-01\" \u003eBorys Melnychuk\u003c/a\u003e\n","\n","\u003ca href=\"https://author.skills.network/instructors/yaroslav_vyklyuk_2?utm_medium=Exinfluencer\u0026utm_source=Exinfluencer\u0026utm_content=000026UJ\u0026utm_term=10006555\u0026utm_id=NA-SkillsNetwork-Channel-SkillsNetworkGuidedProjectsIBMSkillsNetworkGPXX0QGDEN2306-2023-01-01\"\u003eProf. Yaroslav Vyklyuk, DrSc, PhD\u003c/a\u003e\n","\n","\u003ca href=\"https://author.skills.network/instructors/mariya_fleychuk?utm_medium=Exinfluencer\u0026utm_source=Exinfluencer\u0026utm_content=000026UJ\u0026utm_term=10006555\u0026utm_id=NA-SkillsNetwork-Channel-SkillsNetworkGuidedProjectsIBMSkillsNetworkGPXX0QGDEN2306-2023-01-01\"\u003eProf. Mariya Fleychuk, DrSc, PhD\u003c/a\u003e\n","\n","\n","\n","## Change Log\n","\n","| Date (YYYY-MM-DD) | Version | Changed By      | Change Description                                         |\n","| ----------------- | ------- | ----------------| ---------------------------------------------------------- |\n","|     2023-03-25    |   1.0   | Borys Melnychuk | Creation of the lab                                        |\n","\n","\u003chr\u003e\n","\n","## \u003ch3 align=\"center\"\u003e © IBM Corporation 2023. All rights reserved. \u003c/h3\u003e\n"]}],"metadata":{"kernelspec":{"display_name":"Python","language":"python","name":"conda-env-python-py"},"language_info":{"name":""}},"nbformat":4,"nbformat_minor":4}
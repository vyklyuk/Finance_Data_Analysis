{"cells":[{"cell_type":"markdown","id":"c252d90f-5425-4999-8f6d-72b4800ea695","metadata":{},"outputs":[],"source":["\u003ccenter\u003e\n","    \u003cimg src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/assets/logos/SN_web_lightmode.png\" width=\"500\" alt=\"cognitiveclass.ai logo\"\u003e\n","\u003c/center\u003e\n","\n","#  **Investigation of cryptocurrency exchange rate dynamic (on the example of cryptocurrency pair MATIC/BUSD), сalculation and analysis of technical financial indicators, characterizing the cryptocurrency market (the example of ATR, OBV, RSI, AD)**\n"]},{"cell_type":"markdown","id":"24cf7fdd-9e62-4ef5-8a0e-600f757cd473","metadata":{},"outputs":[],"source":["## **Lab 3. Data Analysis with Python**\n","\n","Estimated time needed: **30** minutes\n","\n","## **The tasks**\n","\n","* Explore features or characteristics to predict price of cryptocurrency;\n","* Visualize cryptocurrency dynamics using Candlestick Chart;\n","* Estimate high or low relationships level between cryptocurrency characteristics and indicators;\n","* Perform financial statistic tests.\n","\n","## **Objectives**\n","\n","After completing this lab you will be able to:\n","\n","* find correlation and causation between cryptocurrencies indicators;\n","* group data;\n","* evaluate Durbin-Watson Test, Granger Causality test etc.;\n","* use Analysis of Variance (ANOVA).\n"]},{"cell_type":"markdown","id":"a89a2f28-1052-4572-84f6-2ad6a28bfb4d","metadata":{},"outputs":[],"source":[]},{"cell_type":"markdown","id":"08bc546b-450a-4c12-9e88-851a84456f59","metadata":{},"outputs":[],"source":["## **Table of Contents**\n","\n","\u003cdiv class=\"alert alert-block alert-info\" style=\"margin-top: 20px\"\u003e\n","\u003col\u003e\n","    \u003cli\u003eImport Data\u003c/li\u003e\n","    \u003cli\u003eAnalyzing Individual Feature Patterns using Visualization\u003c/li\u003e\n","    \u003cul\u003e\n","        \u003cli\u003eChoosing the right visualization method\u003c/li\u003e\n","        \u003cli\u003eCandlestick chart\u003c/li\u003e\n","        \u003cli\u003eCorrelation calculation\u003c/li\u003e\n","        \u003cli\u003eContinuous Numerical Variables\u003c/li\u003e\n","        \u003cli\u003eLinear Relationship\u003c/li\u003e\n","        \u003cli\u003eCategorical variables\u003c/li\u003e\n","    \u003c/ul\u003e\n","    \u003cli\u003eDescriptive Statistical Analysis\u003c/li\u003e\n","    \u003cul\u003e\n","        \u003cli\u003eValue Counts\u003c/li\u003e\n","    \u003c/ul\u003e\n","    \u003cli\u003eBasics of Grouping\u003c/li\u003e\n","    \u003cli\u003eCorrelation and Causation\u003c/li\u003e\n","    \u003cul\u003e\n","        \u003cli\u003ePearson Correlation\u003c/li\u003e\n","        \u003cli\u003eP-value\u003c/li\u003e\n","    \u003c/ul\u003e\n","    \u003cli\u003eANOVA: Analysis of Variance\u003c/li\u003e\n","    \u003cli\u003eDurbin-Watson Test\u003c/li\u003e\n","    \u003cli\u003eGranger Causality Test\u003c/li\u003e\n","    \u003cli\u003eSources\u003c/li\u003e\n","\u003c/ol\u003e\n","\n","\u003c/div\u003e\n","\n","\u003chr\u003e\n"]},{"cell_type":"markdown","id":"92d1f163-69be-4c6d-af21-95ac46179f35","metadata":{},"outputs":[],"source":["## **Dataset Description**\n","\n","### **Files**\n","* #### **MATICBUSD_trades_1m_preprocessed.csv** - the file contains historical changes of the pair **MATIC/BUSD** and ATR, OBV, RSI, AD indicators for the period from 11/11/2022 to 12/29/2022 with an aggregation time of 1 minute. **MATIC/BUSD** - the exchange rate of **MATIC** cryptocurrency to **BUSD** cryptocurrency\n","\n","### **Columns**\n","\n","* #### `Ts` - the timestamp of the record\n","* #### `Open` -  the price of the asset at the beginning of the trading period\n","* #### `High` -  the highest price of the asset during the trading period\n","* #### `Low` - the lowest price of the asset during the trading period.\n","* #### `Close` - the price of the asset at the end of the trading period\n","* #### `Volume` - the total number of shares or contracts of a particular asset that are traded during a given period\n","* #### `Rec_count` -  the number of individual trades or transactions that have been executed during a given time period\n","* #### `Avg_price` - the average price at which a particular asset has been bought or sold during a given period\n","* #### `ATR` - average true range indicator\n","* #### `OBV` - on-balance volume indicator\n","* #### `RSI` - relative strength index indicator\n","* #### `AD` - accumulation / distribution indicator\n"]},{"cell_type":"markdown","id":"017502cd-de64-4a31-a9b9-016c9792cd52","metadata":{},"outputs":[],"source":["### How to find the correlation between cryptocurrencies and conduct various tests on them?\n"]},{"cell_type":"markdown","id":"f07086f7-56b8-481f-95f0-959691c749c6","metadata":{},"outputs":[],"source":["# **1. Import Data**\n"]},{"cell_type":"markdown","id":"2ad04af1-322e-4887-a9db-f402b7f231b1","metadata":{},"outputs":[],"source":["If you run the lab locally using Anaconda, you can load the correct library and versions by uncommenting the following:\n"]},{"cell_type":"code","id":"3f1a126b-8ca7-4f4a-8749-c35ce1f991e2","metadata":{},"outputs":[],"source":["# If you run the lab locally using Anaconda, you can load the correct library and versions by uncommenting the following:\n# ! conda install pandas -y\n# ! conda install numpy -y\n# ! conda install scipy -y\n# ! conda install matplotlib -y\n# ! conda install seaborn -y\n! conda install -c conda-forge mplfinance -y\n# ! conda install -c anaconda statsmodels -y"]},{"cell_type":"code","id":"b40f8b34-fa31-4ef2-8f75-ab051c61580f","metadata":{},"outputs":[],"source":["import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport matplotlib.dates as mpl_dates\nimport mplfinance as mpf\nimport statsmodels.api as sm\n\nfrom scipy import stats\nfrom statsmodels.stats.stattools import durbin_watson as dwtest\nfrom statsmodels.tsa.stattools import grangercausalitytests\n\nimport itertools\nimport warnings\nimport datetime as dt\n\npd.set_option(\"display.precision\", 5) # setting numbers after digits\npd.options.display.float_format = '{:.5f}'.format\nwarnings.filterwarnings(\"ignore\") # filterig warnings\nsns.set() # setting theme"]},{"cell_type":"markdown","id":"3ff18c48-46c7-4610-94ed-6cea1348a234","metadata":{},"outputs":[],"source":["We will use dataset that we created in the first lab \"MATICBUSD_trades_1m_preprocessed.csv\"\n"]},{"cell_type":"code","id":"01304c22-85f6-4b25-b521-6aba84bee731","metadata":{},"outputs":[],"source":["filename = \"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMSkillsNetwork-GPXX0CJ2EN/MATICBUSD_trades_1m_preprocessed.csv\""]},{"cell_type":"markdown","id":"58b65b87-22b1-47ca-85f6-7155576f18a4","metadata":{},"outputs":[],"source":["Load the data and store it in dataframe `df`:\n"]},{"cell_type":"code","id":"f6d9f778-3344-4246-9e73-8cc94eb02320","metadata":{},"outputs":[],"source":["df = pd.read_csv(filename, parse_dates=[\"Ts\"]) # We set parameter parse_dates to specify columns which need to perceived as datetime"]},{"cell_type":"markdown","id":"3622bba1-7eb7-4664-a8b4-58f4defd1632","metadata":{},"outputs":[],"source":["This dataset was hosted \u003ca href=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMSkillsNetwork-GPXX0CJ2EN/MATICBUSD_trades_1m_preprocessed.csv\"\u003eHERE\u003c/a\u003e\n"]},{"cell_type":"markdown","id":"764f889d-be02-4d2a-85d2-f79f07e8441f","metadata":{},"outputs":[],"source":["Let's take a look on data that we have got\n"]},{"cell_type":"code","id":"3789bf3c-0aad-4012-852c-5fe06a16894f","metadata":{},"outputs":[],"source":["df.head(20)"]},{"cell_type":"markdown","id":"843670ea-19d7-41cb-bfd2-59e7ca31a14a","metadata":{},"outputs":[],"source":["How we can see our first 15 rows of columns **\"ATR\"** and **\"RSI\"** are `NaN`'s so we need to drop them\n"]},{"cell_type":"code","id":"e0d5d905-bd6e-49fb-8b6f-e8d197861f8e","metadata":{},"outputs":[],"source":["df = df.dropna()"]},{"cell_type":"markdown","id":"c10b2441-2888-4e24-8ff1-93fb0e7599d0","metadata":{},"outputs":[],"source":["# **2. Analyzing Individual Feature Patterns Using Visualization**\n"]},{"cell_type":"markdown","id":"3d07ddd2-f86a-4a58-9d24-e8273e2f7e1c","metadata":{},"outputs":[],"source":["## **Choosing the right visualization method**\n","\n","When visualizing individual variables, it is important to first understand what type of variable you are dealing with. This will help us find the right visualization method for that variable.\n"]},{"cell_type":"code","id":"7ccb1f3e-7994-478c-9306-6479510c3b37","metadata":{},"outputs":[],"source":["# list the data types for each column\nprint(df.dtypes)"]},{"cell_type":"markdown","id":"07139b63-1158-42f1-b64c-eee4b033e604","metadata":{},"outputs":[],"source":["Let's define function that plots **candlestick chart**\n"]},{"cell_type":"code","id":"e1d89141-37b0-4b2a-9ba1-0de7b7e46c0e","metadata":{},"outputs":[],"source":["def plot_candlestick_chart(df: pd.DataFrame, curr: str) -\u003e None:\n    \"\"\"\n    Plots candlestick chart\n    \n    Parameters\n    ----------\n    df: pd.DataFrame\n        Pandas dataframe that needs to contain columns \"Ts\", \"Open\", \"High\", \"Low\", \"Close\", \"Volume\"\n    curr: str\n        Name of currency that `df` contains\n    \"\"\"\n    # Extracting Data for plotting\n    ohlc = df[[\"Ts\", \"Open\", \"High\", \"Low\", \"Close\", \"Volume\"]].copy()\n\n    # Setting \"Ts\" column as datatime if it's not yet\n    ohlc[\"Ts\"] = pd.to_datetime(ohlc[\"Ts\"])\n    ohlc.index = ohlc[\"Ts\"]\n\n    # Resampling to 1 day\n    ohlc = ohlc.resample(\"1d\").agg({\n        \"Open\": \"first\",\n        \"High\": \"max\",\n        \"Low\": \"min\",\n        \"Close\": \"last\",\n        \"Volume\": \"sum\"\n    })\n\n    # Setting \"Ts\" column as index and result in the correct format\n    ohlc[\"Ts\"] = ohlc.index\n    ohlc[\"Ts\"] = ohlc[\"Ts\"].apply(mpl_dates.date2num)\n\n    # Plotting the candlestick chart\n    mpf.plot(ohlc, type=\"candle\", \n             volume=True, \n             style=\"yahoo\", \n             ylabel=\"Price\", \n             xlabel=\"Date\", \n             title=f\"Daily Candlestick Chart of {curr}\")"]},{"cell_type":"markdown","id":"97cea548-49f8-4f45-9f68-4f140d31bd61","metadata":{},"outputs":[],"source":["## **Candlestick chart**\n"]},{"cell_type":"markdown","id":"50edb6a0-4526-44ae-992f-12fe31681749","metadata":{},"outputs":[],"source":["A **candlestick chart** (also called **Japanese candlestick** chart or **K-line**) is a style of financial chart used to describe price movements of a security, derivative, or currency.\n","\n","It is similar to a bar chart in that each candlestick represents all four important pieces of information for that day: open and close in the thick body; high and low in the “candle wick”. Being densely packed with information, it tends to represent trading patterns over short periods of time, often a few days or a few trading sessions.\n","\n","Candlestick charts are most often used in technical analysis of equity and currency price patterns. They are used by traders to determine possible price movement based on past patterns, and who use the opening price, closing price, high and low of that time period. They are visually similar to box plots, though box plots show different information.\n"]},{"cell_type":"markdown","id":"a349adfd-301b-4d6d-9df0-c0d31b84cdf5","metadata":{},"outputs":[],"source":["\u003ccenter\u003e\n","    \u003cimg src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMSkillsNetwork-GPXX0CJ2EN/1024px-Candlestick_chart_scheme_01-en.svg.png\" width=\"300\" alt=\"candlestick\"\u003e\n","\u003c/center\u003e\n"]},{"cell_type":"markdown","id":"adfdb57d-c6c2-45d0-a262-15bd02bd2373","metadata":{},"outputs":[],"source":["Scheme of a single candlestick chart. A candlestick as this one is usually shaded red as the close is lower than the open. The Low and High caps are usually not present but may be added to ease reading.\n"]},{"cell_type":"markdown","id":"4aeb037e-e91a-4fd3-82a5-442660cddd58","metadata":{},"outputs":[],"source":["Let's see our candlestick chart\n"]},{"cell_type":"code","id":"2d06064e-265e-4339-a89f-318d0c8476bb","metadata":{},"outputs":[],"source":["plot_candlestick_chart(df, \"MATIC/BUSD\")"]},{"cell_type":"markdown","id":"4fae3d73-e925-4061-aa99-069c0dd64e85","metadata":{},"outputs":[],"source":["\u003cdiv class=\"alert alert-danger alertdanger\" style=\"margin-top: 20px\"\u003e\n","\u003ch1\u003e\u003cstrong\u003eQuestion  #1:\u003c/strong\u003e\u003c/h1\u003e\n","\n","**What is the data type of the column \"Avg_price\"?**\n","\n","\u003c/div\u003e\n"]},{"cell_type":"code","id":"5ddef203-606b-4baa-9019-5bf83a3ae263","metadata":{},"outputs":[],"source":["# Write your code below and press Shift+Enter to execute \n"]},{"cell_type":"markdown","id":"a5ee09ea-e206-4443-a323-fff93d247041","metadata":{},"outputs":[],"source":["\u003cdetails\u003e\u003csummary\u003eClick here for the solution\u003c/summary\u003e\n","\n","```python\n","df[\"Avg_price\"].dtypes\n","```\n","\n","\u003c/details\u003e\n"]},{"cell_type":"markdown","id":"3f4fe868-dfd9-48f3-98a7-6b86b1fb3b3f","metadata":{},"outputs":[],"source":["## **Correlation calculation**\n"]},{"cell_type":"markdown","id":"e35722aa-6b04-4291-8d83-4399b5329dc8","metadata":{},"outputs":[],"source":["For example, we can calculate the correlation between variables  of type `int64` or `float64` using the method `.corr()`:\n"]},{"cell_type":"code","id":"d126d73d-34c8-4690-8b60-333bee4d6b05","metadata":{},"outputs":[],"source":["corr = df.corr()\ncorr"]},{"cell_type":"markdown","id":"152782b8-1530-4007-9c8b-bafe4d9bb837","metadata":{},"outputs":[],"source":["The diagonal elements are always one; we will study correlation more precisely Pearson correlation in-depth at the end of the notebook.\n"]},{"cell_type":"markdown","id":"c049cc1f-fbd9-4004-843a-e19e51b55cd4","metadata":{},"outputs":[],"source":["We have a dataframe of correlations now we can build **heatmap** based on that correlation to perceive this data visually\n"]},{"cell_type":"markdown","id":"cea99e15-9f73-4981-9b93-1715bb57f6d0","metadata":{},"outputs":[],"source":["**heatmap** plots rectangular data as a color-encoded matrix.\n"]},{"cell_type":"code","id":"eddc470f-468e-4c7a-a202-4d0abb6b0deb","metadata":{},"outputs":[],"source":["sns.heatmap(corr)"]},{"cell_type":"markdown","id":"9168078c-5835-4565-b32c-9050b97707c1","metadata":{},"outputs":[],"source":["\u003cdiv class=\"alert alert-danger alertdanger\" style=\"margin-top: 20px\"\u003e\n","\u003ch1\u003e\u003cstrong\u003eQuestion  #2:\u003c/strong\u003e\u003c/h1\u003e\n","\n","**Find the correlation between the following columns: \"Avg_price\", \"ATR\" and \"AD\".**\u003cbr\u003e\u003cbr\u003e\n","\u003cstrong\u003eHint: if you would like to select those columns, use the following syntax: `df[[\"Avg_price\", \"ATR\", \"AD\"]]`\u003c/strong\u003e\n","\u003c/div\u003e\n"]},{"cell_type":"code","id":"e17ae32a-10a1-4366-9929-f52de9b54913","metadata":{},"outputs":[],"source":["# Write your code below and press Shift+Enter to execute \n"]},{"cell_type":"markdown","id":"47460414-eb42-4fd1-9b55-a897a9ce064d","metadata":{},"outputs":[],"source":["\u003cdetails\u003e\u003csummary\u003eClick here for the solution\u003c/summary\u003e\n","\n","```python\n","df[[\"Avg_price\", \"ATR\", \"AD\"]].corr()\n","```\n","\n","\u003c/details\u003e\n"]},{"cell_type":"markdown","id":"31025887-c289-49b5-938d-ae6edc94de94","metadata":{},"outputs":[],"source":["## **Continuous Numerical Variables**\n","\n","Continuous numerical variables are variables that may contain any value within some range. They can be of type `int64` or `float64`. A great way to visualize these variables is by using scatterplots with fitted lines.\n","\n","In order to start understanding the (linear) relationship between an individual variable and the price, we can use `regplot` which plots the scatterplot plus the fitted regression line for the data.\n"]},{"cell_type":"markdown","id":"64fef4d9-2a7d-4385-99a5-0387b3aed33e","metadata":{},"outputs":[],"source":["Let's see several examples of different linear relationships:\n"]},{"cell_type":"markdown","id":"9359d5bf-b305-4a0e-b5a2-34b958c62d47","metadata":{},"outputs":[],"source":["## **Linear Relationship**\n"]},{"cell_type":"markdown","id":"94a54bcc-6208-4953-8503-8ad40400df7b","metadata":{},"outputs":[],"source":["A **linear relationship** (or **linear association**) is a statistical term used to describe a straight-line relationship between two variables. Linear relationships can be expressed either in a graphical format where the variable and the constant are connected via a straight line or in a mathematical format where the independent variable is multiplied by the slope coefficient, added by a constant, which determines the dependent variable. \n"]},{"cell_type":"markdown","id":"54f2b14a-80ef-491d-94fd-e87decab760f","metadata":{},"outputs":[],"source":["Let's take 2 columns **\"Avg_price\"** and **\"ATR\"** and see how they are correlated\n"]},{"cell_type":"code","id":"84321514-419d-4337-b6ff-7d1893c22885","metadata":{},"outputs":[],"source":["# ATR as potential predictor variable of Avg_price\nsns.regplot(x=\"ATR\", y=\"Avg_price\", data=df)"]},{"cell_type":"markdown","id":"bdd9c88e-7292-4529-82b5-81250ed993cd","metadata":{},"outputs":[],"source":["We can see that at small values of **\"ATR\"** the correlation is quite good, but as **\"ATR\"** increases the correlation decreases\n"]},{"cell_type":"code","id":"66d21214-026d-4872-a0c8-5c605cc6b10d","metadata":{},"outputs":[],"source":["df[[\"ATR\", \"Avg_price\"]].corr()"]},{"cell_type":"markdown","id":"1c99481b-61fe-4fe2-ae50-7c7dc4e35efc","metadata":{},"outputs":[],"source":["We can examine the correlation between **\"ATR\"** and **\"Avg_price\"** and see that it's approximately 0.33449\n"]},{"cell_type":"markdown","id":"69de03d3-90fc-4b7e-947f-8762226ab342","metadata":{},"outputs":[],"source":["Let's find the scatterplot of **\"OBV\"** and **\"Avg_price\"**\n"]},{"cell_type":"code","id":"f3c34717-639f-4bc6-b46c-73af7d0786f7","metadata":{},"outputs":[],"source":["sns.regplot(x=\"OBV\", y=\"Avg_price\", data=df)"]},{"cell_type":"markdown","id":"9e524511-44f2-43b8-89fa-500028d1922a","metadata":{},"outputs":[],"source":["**\"OBV\"** does not seem like a good predictor of **\"Avg_price\"** since the regression line is close to horizontal and in most cases the data points are located far from the fitted line. \n"]},{"cell_type":"code","id":"d2357dce-e9e5-4ebf-aefd-658a42b9169f","metadata":{},"outputs":[],"source":["df[[\"OBV\", \"Avg_price\"]].corr()"]},{"cell_type":"markdown","id":"d07b1313-72d8-43de-9a7c-33ee4b042206","metadata":{},"outputs":[],"source":["We can examine the correlation between **\"OBV\"** and **\"Avg_price\"** and see that it's approximately 0.19047\n"]},{"cell_type":"markdown","id":"ae9ea687-cdf4-4c47-84f9-35dbd9988f04","metadata":{},"outputs":[],"source":["Let's find the scatterplot of **\"AD\"** and **\"Avg_price\"**\n"]},{"cell_type":"code","id":"3e7ee336-5009-4400-9ccc-be766a4393d5","metadata":{},"outputs":[],"source":["sns.regplot(x=\"AD\", y=\"Avg_price\", data=df)"]},{"cell_type":"markdown","id":"d58ff89b-c1f7-4c13-ac44-0abbbc9f9ee9","metadata":{},"outputs":[],"source":["\u003cp\u003eWe see that there is a fairly conditional correlation. in some cases the points are located close to the straight line, in some - not. \u003c/p\u003e\n"]},{"cell_type":"code","id":"045c4a6f-bbec-4ced-b0db-4d1d5ddbd6a1","metadata":{},"outputs":[],"source":["df[[\"AD\", \"Avg_price\"]].corr()"]},{"cell_type":"markdown","id":"ca61d431-075b-42e5-9ed8-bb4b98682c19","metadata":{},"outputs":[],"source":["We can examine the correlation between **\"AD\"** and **\"Avg_price\"** and see that it's approximately 0.48184\n"]},{"cell_type":"markdown","id":"bbb45e8c-07d3-4b4a-ad00-4dc889fb8705","metadata":{},"outputs":[],"source":["\u003cdiv class=\"alert alert-danger alertdanger\" style=\"margin-top: 20px\"\u003e\n","\u003ch1\u003e\u003cstrong\u003eQuestion  3 a):\u003c/strong\u003e\u003c/h1\u003e\n","\n","\u003cstrong\u003eFind the correlation  between \"Avg_price\" and \"RSI\".\u003c/strong\u003e\u003cbr\u003e\u003cbr\u003e\n","\u003cstrong\u003eHint: if you would like to select those columns, use the following syntax: df[[\"Avg_price\", \"RSI\"]]\u003c/strong\u003e\n","\u003c/div\u003e\n"]},{"cell_type":"code","id":"fdf98ef8-271e-4e58-a357-46f37c03b4ef","metadata":{},"outputs":[],"source":["# Write your code below and press Shift+Enter to execute\n"]},{"cell_type":"markdown","id":"b961d1a1-89e3-46f6-bc9e-3d49b6123fde","metadata":{},"outputs":[],"source":["\u003cdetails\u003e\u003csummary\u003eClick here for the solution\u003c/summary\u003e\n","\n","```python\n","\n","#The correlation is 0.0298, the non-diagonal elements of the table.\n","\n","df[[\"Avg_price\", \"RSI\"]].corr()\n","\n","```\n","\n","\u003c/details\u003e\n"]},{"cell_type":"markdown","id":"6d0b74f8-3427-4414-a9f6-c7b715d788de","metadata":{},"outputs":[],"source":["\u003cdiv class=\"alert alert-danger alertdanger\" style=\"margin-top: 20px\"\u003e\n","\u003ch1\u003e\u003cstrong\u003eQuestion  3 b):\u003c/strong\u003e\u003c/h1\u003e\n","\n","**Given the correlation results between \"Avg_price\" and \"RSI\", do you expect a linear relationship?**\n","**Verify your results using the function `regplot()`**\n","\u003c/div\u003e\n"]},{"cell_type":"code","id":"b263f3db-b1da-4128-9ae0-285dfa4ae22c","metadata":{},"outputs":[],"source":["# Write your code below and press Shift+Enter to execute \n"]},{"cell_type":"markdown","id":"fd1b2c88-c123-497d-add7-e41207d4d4d8","metadata":{},"outputs":[],"source":["\u003cdetails\u003e\u003csummary\u003eClick here for the solution\u003c/summary\u003e\n","\n","```python\n","\n","# There is no correlation between the variables \"RSI\" and \"Avg_price\". We can see this using \"regplot\" to demonstrate this.\n","\n","# Code: \n","sns.regplot(x=\"Avg_price\", y=\"RSI\", data=df)\n","\n","```\n","\n","\u003c/details\u003e\n"]},{"cell_type":"markdown","id":"9b9750e1-5496-471d-a590-ba3655a3437e","metadata":{},"outputs":[],"source":["## **Categorical Variables**\n","\n","These are variables that describe a \"characteristic\" of a data unit, and are selected from a small group of categories. The categorical variables can have the type `object` or `int64`. A good way to visualize categorical variables is by using boxplots.\n"]},{"cell_type":"markdown","id":"3a67d646-e1c9-4fd1-b7d5-c4df81f5ddb0","metadata":{},"outputs":[],"source":["But firstly we need to create the categories. Let's split \"Avg_price\" into 5 categories (Low, Lower Medium, Medium, Upper Medium, High). To do this we will use `np.linspace` and `pd.cut`. We used `pd.cut` in second lab. `np.linspace` returns evenly spaced numbers over a specified interval.\n"]},{"cell_type":"code","id":"f31a1c24-4e13-48c2-b274-96ac11991732","metadata":{},"outputs":[],"source":["group_names = [\"Low\", \"Lower Medium\", \"Medium\", \"Upper Medium\", \"High\"]"]},{"cell_type":"markdown","id":"ed1274f1-bf14-4e87-80a2-84a45e607068","metadata":{},"outputs":[],"source":["Let's define a function which convert absolute values to categorical ones\n"]},{"cell_type":"code","id":"a66137bb-017c-4f24-bc9f-80575dc05487","metadata":{},"outputs":[],"source":["def to_categorical(column: pd.Series, labels: list) -\u003e pd.Series:\n    \"\"\"\n    Convert `column` into categorical pd.Series with labels given as parameter `labels`\n    \n    Parameters\n    ----------\n    column: pd.Series\n        Column to convert\n    labels: list\n        Labels which we will use as categories\n    \n    Returns\n    -------\n    res: pd.Series\n        Categorical column\n    \"\"\"\n    cat_number = len(labels)\n    bins = np.linspace(min(column), max(column), cat_number+1)\n    res = pd.cut(column, bins, labels=labels, include_lowest=True)\n    return res"]},{"cell_type":"code","id":"91383993-a13a-4658-8e4e-221488b079e2","metadata":{},"outputs":[],"source":["df[\"ap_cat\"] = to_categorical(df[\"Avg_price\"], group_names)\ndf[[\"ap_cat\", \"Avg_price\"]].tail()"]},{"cell_type":"markdown","id":"bfa74533-b5dc-420b-864e-3bb83e7c8249","metadata":{},"outputs":[],"source":["Now let's do the same with **\"AD\"**\n"]},{"cell_type":"code","id":"64ae0abf-9cb0-4388-a7cf-056dd0243c59","metadata":{},"outputs":[],"source":["df[\"AD_cat\"] = to_categorical(df[\"AD\"], group_names)"]},{"cell_type":"markdown","id":"a26fe6d7-f74c-47e9-a9a1-aa87c7c682da","metadata":{},"outputs":[],"source":["Let's look at the relationship between **\"Avg_price\"** and **\"ap_cat\"**.\n"]},{"cell_type":"code","id":"156bab61-d149-4970-bd54-0c4e86737f24","metadata":{},"outputs":[],"source":["sns.boxplot(x=\"Avg_price\", y=\"ap_cat\", data=df)"]},{"cell_type":"markdown","id":"aa52209f-eeb5-4b16-b225-6e26c564df27","metadata":{},"outputs":[],"source":["Here we see that the distribution of price between these five categories, Low, Lower Medium, Medium, Upper Medium and High.\n"]},{"cell_type":"markdown","id":"25a3bf01-8d28-4d37-a7f5-613aebb35ade","metadata":{},"outputs":[],"source":["# **3. Descriptive Statistical Analysis**\n"]},{"cell_type":"markdown","id":"3b28905a-9d18-43b2-a9af-da90d228e0f4","metadata":{},"outputs":[],"source":["Let's first take a look at the variables by utilizing a description method.\n","\n","The `.describe()` function automatically computes basic statistics for all continuous variables. Any `NaN` values are automatically skipped in these statistics.\n","\n","This will show:\n","\n","* the count of that variable\n","* the mean\n","* the standard deviation (std)\n","* the minimum value\n","* the IQR (Interquartile Range: 25%, 50% and 75%)\n","* the maximum value\n"]},{"cell_type":"markdown","id":"dccce9b6-ce59-45b7-837c-7dfce4d978e3","metadata":{},"outputs":[],"source":["We can apply the method `.describe()` as follows:\n"]},{"cell_type":"code","id":"b5bde0f5-58b8-4bc6-9a91-ca8845f55b01","metadata":{},"outputs":[],"source":["df.describe()"]},{"cell_type":"markdown","id":"de3f0286-1473-4184-b04c-3aa4aa67fc30","metadata":{},"outputs":[],"source":["However `.describe()` does not include categorical columns so let's include them specifying the `include` parameter\n"]},{"cell_type":"code","id":"e50ef978-4c54-415d-a54d-3e7e650517d4","metadata":{},"outputs":[],"source":["df.describe(include=\"category\")"]},{"cell_type":"markdown","id":"d8c48c88-11ba-4c2c-84ef-3929f1493b5b","metadata":{},"outputs":[],"source":["## **Value Counts**\n"]},{"cell_type":"markdown","id":"2a3ccc4c-5ea8-45ab-8737-591ab71961ee","metadata":{},"outputs":[],"source":["Value counts is a good way of understanding how many units of each characteristic/variable we have. We can apply the `.value_counts()` method on the column **\"ap_cat\"**. Don’t forget the method `.value_counts()` only works on pandas series, not pandas dataframes. As a result, we only include one bracket `df[\"ap_cat\"]`, not two brackets `df[[\"ap_cat\"]]`\n"]},{"cell_type":"code","id":"4af6c06d-65e8-400b-9be4-c8e7027748dd","metadata":{},"outputs":[],"source":["df[\"ap_cat\"].value_counts()"]},{"cell_type":"markdown","id":"68dd0b6b-49d4-4b95-be2f-f8e7fe38962d","metadata":{},"outputs":[],"source":["We can convert the series to a dataframe as follows:\n"]},{"cell_type":"code","id":"8388f31b-d429-47c0-9e10-130084b59494","metadata":{},"outputs":[],"source":["df[\"ap_cat\"].value_counts().to_frame()"]},{"cell_type":"markdown","id":"6a2f87e3-e22a-47d5-a1c2-e982f485c90a","metadata":{},"outputs":[],"source":["Let's repeat the above steps but save the results to the dataframe **\"ap_cat\"** and rename the column **\"ap_cat\"** to `.value_counts()`\n"]},{"cell_type":"code","id":"9ff8cb3e-b678-4abe-bee3-dcb043d3e43f","metadata":{},"outputs":[],"source":["ap_counts = df[\"ap_cat\"].value_counts().to_frame()\nap_counts.rename(columns={\"ap_cat\": \"value_counts\"}, inplace=True)\nap_counts"]},{"cell_type":"markdown","id":"ec30b5ee-85da-4a30-8db0-edc4fdd65eb8","metadata":{},"outputs":[],"source":["Now let's rename the index to **\"ap_cat\"**:\n"]},{"cell_type":"code","id":"1c708151-9e94-455b-a6de-d5551b1f57ad","metadata":{},"outputs":[],"source":["ap_counts.index.name = \"ap_cat\"\nap_counts"]},{"cell_type":"markdown","id":"48e5581c-5019-4a34-8dea-add39a22c1f8","metadata":{},"outputs":[],"source":["We can repeat the above process for the variable **\"AD_cat\"**.\n"]},{"cell_type":"code","id":"cd0ea11d-4d5a-4d5c-b07e-3768b78606c5","metadata":{},"outputs":[],"source":["# AD_cat as variable\nad_cat = df[\"AD_cat\"].value_counts().to_frame()\nad_cat.rename(columns={\"AD_cat\": \"value_counts\"}, inplace=True)\nad_cat.index.name = \"AD_cat\"\nad_cat.head(10)"]},{"cell_type":"markdown","id":"442eb0d3-bf4c-4a0d-8fb2-541579632e52","metadata":{},"outputs":[],"source":["# **4. Basics of Grouping**\n"]},{"cell_type":"markdown","id":"65e5b894-6f93-476c-89e2-df69d03807bb","metadata":{},"outputs":[],"source":["The `groupby()` method groups data by different categories. The data is grouped based on one or several variables, and analysis is performed on the individual groups\n","\n","For example, let's group by the variable **\"ap_cat\"**. We see that there are 5 different categories\n"]},{"cell_type":"code","id":"1ef73449-60bd-474a-a26a-f517a9a444d6","metadata":{},"outputs":[],"source":["df[\"ap_cat\"].unique()"]},{"cell_type":"markdown","id":"6c5fdc29-16e4-4b10-8c4c-3dff86a9b1ad","metadata":{},"outputs":[],"source":["If we want to know, on average, which type of **\"ap_cat\"** is most valuable, we can group **\"ap_cat\"** and then average them.\n","\n","We can select the columns **\"ap_cat\"** and **\"Avg_price\"**, then assign it to the variable `df_group_one`\n"]},{"cell_type":"code","id":"03c73c18-3a13-4571-9a41-7f2eca83d547","metadata":{},"outputs":[],"source":["df_group_one = df[[\"ap_cat\", \"Avg_price\"]]"]},{"cell_type":"markdown","id":"a53f5020-4eb9-4c33-9bb5-b44578133034","metadata":{},"outputs":[],"source":["We can then calculate the average price for each of the different categories of data\n"]},{"cell_type":"code","id":"82d210ce-1a8e-46b5-8aa8-5582ac59c2ab","metadata":{},"outputs":[],"source":["# grouping results\ndf_group_one = df_group_one.groupby([\"ap_cat\"] ,as_index=False).mean()\ndf_group_one"]},{"cell_type":"markdown","id":"ba0afc0d-fef2-4bc4-8c8f-40656eb62bee","metadata":{},"outputs":[],"source":["Obviously, High category is, on average, the most expensive\n","\n","You can also group by multiple variables. For example, let's group by both **\"ap_cat\"** and **\"AD_cat\"**. This groups the dataframe by the unique combination of **\"ap_cat\"** and **\"AD_cat\"**. We can store the results in the variable `grouped_test1`\n"]},{"cell_type":"code","id":"da0ef732-5728-442d-9a27-d28d767bc638","metadata":{},"outputs":[],"source":["# grouping results\ndf_gptest = df[[\"ap_cat\", \"AD_cat\", \"Avg_price\"]]\ngrouped_test1 = df_gptest.groupby([\"ap_cat\", \"AD_cat\"], as_index=False).mean()\ngrouped_test1"]},{"cell_type":"markdown","id":"3238f5e2-35da-494b-9672-e5fd614cf5cd","metadata":{},"outputs":[],"source":["This grouped data is much easier to visualize when it is made into a **cross table**. A cross table is a two-way table consisting of columns and rows. It is also known as a pivot table or a multi-dimensional table. Its greatest strength is its ability to structure, summarize and display large amounts of data. Cross tables can also be used to determine whether there is a relation between the row variable and the column variable or not.\n","\n","In this case, we will leave the **\"ap_cat\"** variable as the rows of the table, and **\"AD_cat\"** to become the columns of the table:\n"]},{"cell_type":"code","id":"9d815f51-afdb-4759-b155-390ffdfc83a2","metadata":{},"outputs":[],"source":["crossed_table = pd.crosstab(df[\"ap_cat\"], df[\"AD_cat\"])\ncrossed_table"]},{"cell_type":"markdown","id":"f146a617-ea09-4b85-91fa-232e21b6bdb4","metadata":{},"outputs":[],"source":["\u003cdiv class=\"alert alert-danger alertdanger\" style=\"margin-top: 20px\"\u003e\n","\u003ch1\u003e\u003cstrong\u003eQuestion 4:\u003c/strong\u003e\u003c/h1\u003e\n","\n","**Use the `groupby()` function to find the average price of each category based on \"AD_cat\"**\n","\u003c/div\u003e\n"]},{"cell_type":"code","id":"55fa0ae4-dd85-43fd-b004-79073e58a5ce","metadata":{},"outputs":[],"source":["# Write your code below and press Shift+Enter to execute \n"]},{"cell_type":"markdown","id":"de4a9753-7838-4c94-a3db-c064c043c368","metadata":{},"outputs":[],"source":["\u003cdetails\u003e\u003csummary\u003eClick here for the solution\u003c/summary\u003e\n","\n","```python\n","# grouping results\n","df_gptest2 = df[[\"AD_cat\", \"Avg_price\"]]\n","grouped_test_bodystyle = df_gptest2.groupby([\"AD_cat\"], as_index= False).mean()\n","grouped_test_bodystyle\n","\n","```\n","\n","\u003c/details\u003e\n"]},{"cell_type":"markdown","id":"0875d53d-5a65-4480-8ab7-5ff74a14f507","metadata":{},"outputs":[],"source":["Variables: **\"ap_cat\"** vs **\"AD_cat\"**\n"]},{"cell_type":"markdown","id":"0f4732bc-3c05-43ea-a1bd-257146893544","metadata":{},"outputs":[],"source":["Let's use a heat map to visualize the relationship between **\"ap_cat\"** vs **\"AD_cat\"**\n"]},{"cell_type":"code","id":"b479cd10-a77a-4f57-b1b1-e87616e3ce50","metadata":{},"outputs":[],"source":["# use the grouped results\nplt.pcolor(crossed_table, cmap=\"RdBu\")\nplt.colorbar()\nplt.show()"]},{"cell_type":"markdown","id":"30f577a9-6360-4ddb-bb3b-c8a64651c54d","metadata":{},"outputs":[],"source":["The heatmap plots relationship between these 2 variables. The larger the diagonal elements, the more these two variables are dependent.\n","\n","The default labels convey no useful information to us. Let's change that:\n"]},{"cell_type":"code","id":"973972c1-c764-4302-9c4a-b1e9f98b7c75","metadata":{},"outputs":[],"source":["fig, ax = plt.subplots()\nim = ax.pcolor(crossed_table, cmap=\"RdBu\")\n\n# label names\nrow_labels = crossed_table.columns.categories\ncol_labels = crossed_table.index\n\n# move ticks and labels to the center\nax.set_xticks(np.arange(crossed_table.shape[1]) + 0.5, minor=False)\nax.set_yticks(np.arange(crossed_table.shape[0]) + 0.5, minor=False)\n\n# insert labels\nax.set_xticklabels(row_labels, minor=False)\nax.set_yticklabels(col_labels, minor=False)\n\n# rotate label if too long\nplt.xticks(rotation=90)\n\nfig.colorbar(im)\nplt.show()"]},{"cell_type":"markdown","id":"5ce2f6ac-8df5-4100-ad74-4ceae899f46d","metadata":{},"outputs":[],"source":["Visualization is very important in data science, and Python visualization packages provide great freedom. We will go more in-depth in a separate Python visualizations course\n","\n","To get a better measure of the important characteristics, we look at the correlation of these variables with the price. In other words: how is the **\"Avg_price\"** dependent on other variables?\n"]},{"cell_type":"markdown","id":"c8b649fb-2b98-404b-a104-022623680d67","metadata":{},"outputs":[],"source":["# **5. Correlation and Causation**\n"]},{"cell_type":"markdown","id":"0e9c7b60-e9a5-44a4-8cce-d2d9f3a1f0c2","metadata":{},"outputs":[],"source":["**Correlation**: a measure of the extent of interdependence between variables.\n","\n","**Causation**: the relationship between cause and effect between two variables.\n","\n","It is important to know the difference between these two. Correlation does not imply causation. Determining correlation is much simpler  the determining causation as causation may require independent experimentation.\n"]},{"cell_type":"markdown","id":"435e48e0-fa68-4b04-a822-da6afa4a4453","metadata":{},"outputs":[],"source":["Correlations are useful because they can indicate a predictive relationship that can be exploited in practice. For example, an electrical utility may produce less power on a mild day based on the **correlation** between electricity demand and weather. In this example, there is a causal relationship, because extreme weather causes people to use more electricity for heating or cooling. However, in general, the presence of a correlation is not sufficient to infer the presence of a causal relationship (i.e., correlation does not imply causation). \n"]},{"cell_type":"markdown","id":"f284dcb6-0fad-466d-9576-ad9fc7ee631e","metadata":{},"outputs":[],"source":["## **Pearson Correlation**\n","\n","The Pearson Correlation measures the linear dependence between two variables $X$ and $Y$. The Pearson correlation coefficient attempts to establish a line of best fit through a dataset of two variables by essentially laying out the expected values and the resulting Pearson's correlation coefficient indicates how far away the actual dataset is from the expected values. Depending on the sign of our Pearson's correlation coefficient, we can end up with either a negative or positive correlation if there is any sort of relationship between the variables of our data set.\u003c/p\u003e\n","The resulting coefficient is a value between -1 and 1 inclusive, where:\n","\n","* **1**: Perfect positive linear correlation.\n","* **0**: No linear correlation, the two variables most likely do not affect each other.\n","* **1**: Perfect negative linear correlation.\n"]},{"cell_type":"markdown","id":"8bf8844d-6bfa-43be-8a2e-6bb96bc6e809","metadata":{},"outputs":[],"source":["The population correlation coefficient $ \\rho_{X,Y}$ between two random variables $X$ and $Y$ with expected values $\\mu _{X}$ and $\\mu _{Y}$ and standard deviations $\\sigma _{X}$ and $\\sigma_Y$ is defined as:\n","\n","\u003ccenter\u003e\u003ch1\u003e$\\rho_{X,Y} = \\operatorname{corr}(X, Y) = \\frac{\\operatorname{cov}(X,Y)}{\\sigma _{X} \\sigma_Y} = \\frac{\\operatorname{E}[(X \\; - \\; \\mu_{X})(Y \\; - \\; \\mu_{Y})]}{\\sigma _{X} \\sigma_Y}, \\quad \\text{if} \\; \\sigma_{X} \\sigma_Y \u003e 0 $\u003c/h1\u003e\u003c/center\u003e\n","\n","where $\\operatorname{E}$ is the expected value operator, $\\operatorname{cov}$ means covariance, and $\\operatorname {corr}$ is a widely used alternative notation for the correlation coefficient. The Pearson correlation is defined only if both standard deviations are finite and positive. An alternative formula purely in terms of moments is:\n","\n","\u003ccenter\u003e\u003ch1\u003e$\\rho_{X,Y} = \\frac{\\operatorname{E}(XY) \\; - \\; \\operatorname{E}(X) \\operatorname{E}(Y)}{\\sqrt{\\operatorname{E}(X^{2}) \\; - \\; \\operatorname{E}(X)^2} \\sqrt{\\operatorname{E}(Y^{2}) \\; - \\; \\operatorname{E}(Y)^2}}$\u003c/h3\u003e\u003c/center\u003e\u003cbr\u003e\n"]},{"cell_type":"markdown","id":"71637c51-8341-4603-b9fb-08192f618b92","metadata":{},"outputs":[],"source":["\u003ccenter\u003e\u003cimg width=\"700\" src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMSkillsNetwork-GPXX0CJ2EN/R%20SVG%20Plot.svg\"\u003e\u003c/center\u003e\n","Several sets of $(x, y)$ points, with the Pearson correlation coefficient of $x$ and $y$ for each set. The correlation reflects the noisiness and direction of a linear relationship (top row), but not the slope of that relationship (middle), nor many aspects of nonlinear relationships (bottom). N.B.: the figure in the center has a slope of 0 but in that case, the correlation coefficient is undefined because the variance of $Y$ is zero. \u003cbr\u003e\n"]},{"cell_type":"markdown","id":"76011813-2cea-4993-b6b8-7231802fb349","metadata":{},"outputs":[],"source":["Pearson Correlation is the default method of the function `.corr()`. Like before, we can calculate the Pearson Correlation of the of the `int64` or `float64` variables\n"]},{"cell_type":"code","id":"6da8c5ec-c739-4a3f-bd20-c4dd34e11f8b","metadata":{},"outputs":[],"source":["df.corr()"]},{"cell_type":"markdown","id":"a63db77e-da57-45a7-bc40-2a8cc9bcaa94","metadata":{},"outputs":[],"source":["Sometimes we would like to know the significant of the correlation estimate.\n"]},{"cell_type":"markdown","id":"55cd436c-f40b-460d-ae11-ff0f1d020b28","metadata":{},"outputs":[],"source":["## **P-value**\n","\n","\u003cp\u003eWhat is this P-value? The P-value is the probability value that the correlation between these two variables is statistically significant. Normally, we choose a significance level of 0.05, which means that we are 95% confident that the correlation between the variables is significant.\u003c/p\u003e\n","\n","By convention, when the\n","\n","\u003cul\u003e\n","    \u003cli\u003ep-value is $\u003c$ 0.001: we say there is strong evidence that the correlation is significant.\u003c/li\u003e\n","    \u003cli\u003ethe p-value is $\u003c$ 0.05: there is moderate evidence that the correlation is significant.\u003c/li\u003e\n","    \u003cli\u003ethe p-value is $\u003c$ 0.1: there is weak evidence that the correlation is significant.\u003c/li\u003e\n","    \u003cli\u003ethe p-value is $\u003e$ 0.1: there is no evidence that the correlation is significant.\u003c/li\u003e\n","\u003c/ul\u003e\n"]},{"cell_type":"markdown","id":"6f1ed679-635f-4754-b188-7854dc67ed73","metadata":{},"outputs":[],"source":["Let's calculate the  Pearson Correlation Coefficient and P-value of \"Avg_price\" and \"ATR\", \"OBV\", \"RSI\", \"AD\".\n"]},{"cell_type":"code","id":"3c431b06-e6c3-4df6-b637-b90dc5e191c4","metadata":{},"outputs":[],"source":["indicators = [\"ATR\", \"OBV\", \"RSI\", \"AD\"] # indicators that we want to calculate PCS and p-value with \"Avg_price\"\n\nperformance = pd.DataFrame({\"pair\": [], \"PCS\": [], \"P-value\": []}) # PCS (Pearson Correlation Coefficient)\n\n# Iterating over all the indicators and calculating needed characteristics\nfor indicator in indicators:\n    pearson_coef, p_value = stats.pearsonr(df[\"Avg_price\"], df[indicator])\n    pair = f\"Avg_price, {indicator}\"\n    performance.loc[len(performance.index)] = [pair, pearson_coef, p_value]\n   \n# Printing results\nperformance.sort_values(by=[\"PCS\"], ascending=False).head()"]},{"cell_type":"markdown","id":"3fba9607-bff5-40e1-8256-15363c790793","metadata":{},"outputs":[],"source":["## **Conclusion:**\n","\n","Since the p-value for all pairs is $\u003c$ 0.001 we say there is strong evidence that the correlation is significant. Only correlaton of **\"Avg_price\"** and **\"AD\"** is moderate, all others are weak\n"]},{"cell_type":"markdown","id":"33239b2c-f1c1-4f34-863f-59fe1b526d3d","metadata":{},"outputs":[],"source":["# **6. ANOVA: Analysis of Variance**\n"]},{"cell_type":"markdown","id":"695b7db3-ea46-484e-bc72-17d476e19537","metadata":{},"outputs":[],"source":["The Analysis of Variance  (ANOVA) is a statistical method used to test whether there are significant differences between the means of two or more groups. ANOVA returns two parameters:\n","\n","ANOVA is used in the analysis of comparative experiments, those in which only the difference in outcomes is of interest. The statistical significance of the experiment is determined by a ratio of two variances. This ratio is independent of several possible alterations to the experimental observations: Adding a constant to all observations does not alter significance. Multiplying all observations by a constant does not alter significance. So ANOVA statistical significance result is independent of constant bias and scaling errors as well as the units used in expressing observations. In the era of mechanical calculation it was common to subtract a constant from all observations (when equivalent to dropping leading digits) to simplify data entry. This is an example of data coding.\n","\n","\n","**F-test score**: ANOVA assumes the means of all groups are the same, calculates how much the actual means deviate from the assumption, and reports it as the F-test score. A larger score means there is a larger difference between the means.\n","\n","**P-value**:  P-value tells how statistically significant our calculated score value is.\n"]},{"cell_type":"markdown","id":"eeb3fee9-22f8-4d7c-b873-79a22e7006ba","metadata":{},"outputs":[],"source":["\u003ccenter\u003e\u003ch1\u003e$F = \\frac{MST}{MSE}$\u003c/h1\u003e\u003c/center\u003e\n"]},{"cell_type":"markdown","id":"dedbdb1d-0551-4de9-825e-78bbbf901ce1","metadata":{},"outputs":[],"source":["$\\text{where:}$ \u003cbr\u003e\n","$F = \\text{F-test score}$\u003cbr\u003e\n","$MST = \\text{Mean sum of squares due to treatment}$\u003cbr\u003e\n","$MSE = \\text{Mean sum of squares due to error}$\n"]},{"cell_type":"markdown","id":"c2aef56d-38ce-4daa-b38b-579637458639","metadata":{},"outputs":[],"source":["If our price variable is strongly correlated with the variable we are analyzing, we expect ANOVA to return a sizeable F-test score and a small p-value.\n"]},{"cell_type":"markdown","id":"1e7ccd0d-caa3-4c24-97af-59773201e833","metadata":{},"outputs":[],"source":["Since ANOVA analyzes the difference between different groups of the same variable, the groupby function will come in handy. Because the ANOVA algorithm averages the data automatically, we do not need to take the average before hand.\n"]},{"cell_type":"code","id":"b57e2e62-cebe-484a-81b0-c5ed0038dfca","metadata":{},"outputs":[],"source":["grouped_test2 = df[[\"ap_cat\", \"Avg_price\"]].groupby([\"ap_cat\"])\ngrouped_test2.head()"]},{"cell_type":"markdown","id":"4b1a3668-9638-43ed-8070-d81962695054","metadata":{},"outputs":[],"source":["We can obtain the values of the method group using the method `.get_group()`.\n"]},{"cell_type":"code","id":"e67b0d9d-f88b-4147-b5b9-1fe85f6473fc","metadata":{},"outputs":[],"source":["grouped_test2.get_group(\"Medium\")[\"Avg_price\"]"]},{"cell_type":"markdown","id":"a71ef3b6-d251-47e3-8855-e1fc330554aa","metadata":{},"outputs":[],"source":["We can use the function `stats.f_oneway` in the module `stats` to obtain the **F-test score** and **P-value**.\n"]},{"cell_type":"code","id":"9102fb3c-8321-4149-8c51-78902b10d693","metadata":{},"outputs":[],"source":["# ANOVA\nf_val, p_val = stats.f_oneway(grouped_test2.get_group(\"Low\")[\"Avg_price\"], grouped_test2.get_group(\"Lower Medium\")[\"Avg_price\"], grouped_test2.get_group(\"Medium\")[\"Avg_price\"], grouped_test2.get_group(\"Upper Medium\")[\"Avg_price\"], grouped_test2.get_group(\"High\")[\"Avg_price\"])  \n\nprint(\"ANOVA results: F=%.2f\" % f_val, \", P =\", p_val)"]},{"cell_type":"markdown","id":"60a40760-d225-4841-87cd-3d0f96324165","metadata":{},"outputs":[],"source":["This is a great result with a large F-test score showing a strong correlation and a P-value of 0 implying almost certain statistical significance. But does this mean all five tested groups are all this highly correlated?\n","\n","Let's examine them separately.\n"]},{"cell_type":"code","id":"4c74bfc5-3793-4d00-8095-d631c6cf52ed","metadata":{},"outputs":[],"source":["performance_anova = pd.DataFrame({\"pair\": [], \"F-test\": [], \"P-value\": []})\n\n# Iterating over all the groups and calculating needed characteristics\nfor comb in itertools.combinations(group_names, 2):\n    f_val, p_val = stats.f_oneway(grouped_test2.get_group(comb[0])[\"Avg_price\"], grouped_test2.get_group(comb[1])[\"Avg_price\"])\n    pair = f\"{comb[0]}, {comb[1]}\"\n    performance_anova.loc[len(performance_anova.index)] = [pair, f_val, p_val]\n   \n# Printing results\nperformance_anova.sort_values(by=[\"F-test\"], ascending=False)"]},{"cell_type":"markdown","id":"5b4594f2-379d-466a-b5be-b9f248e11582","metadata":{},"outputs":[],"source":["## **Conclusion:**\n","\n","Every pair of group has p-value of 0 what means that our calculated score value is significant. Every pair has high **F-score** but Low, Medium pairs have the highest of 780563.41995\n"]},{"cell_type":"markdown","id":"a2d08bcd-2e5d-4677-bf5a-adb79b8ba1aa","metadata":{},"outputs":[],"source":["\u003cdiv class=\"alert alert-danger alertdanger\" style=\"margin-top: 20px\"\u003e\n","\u003ch1\u003e\u003cstrong\u003eQuestion 5:\u003c/strong\u003e\u003c/h1\u003e\n","\n","**Get ANOVA score using `stats.f_oneway` function between \"Low\" and \"Medium\" groups**\n","\u003c/div\u003e\n"]},{"cell_type":"code","id":"a68c099c-cbae-4d47-8f5e-25e28c737627","metadata":{},"outputs":[],"source":["# Write your code below and press Shift+Enter to execute \n"]},{"cell_type":"markdown","id":"0a25fb46-28bf-4d50-883c-baae6227ac73","metadata":{},"outputs":[],"source":["\u003cdetails\u003e\u003csummary\u003eClick here for the solution\u003c/summary\u003e\n","\n","```python\n","f_val, p_val = stats.f_oneway(grouped_test2.get_group(\"Low\")[\"Avg_price\"], grouped_test2.get_group(\"Medium\")[\"Avg_price\"])  \n","\n","print(\"ANOVA results: F=%.2f\" % f_val, \", P =\", p_val)\n","\n","```\n","\n","\u003c/details\u003e\n"]},{"cell_type":"markdown","id":"5128c7d0-a203-4e27-98b8-69a6932d7e17","metadata":{},"outputs":[],"source":["# **7. Durbin-Watson Test**\n"]},{"cell_type":"markdown","id":"a3da96ca-04cf-4cab-b25f-8835627d57d5","metadata":{},"outputs":[],"source":["In regression analysis, Durbin-Watson (DW) is useful for checking the first-order autocorrelation (serial correlation). It analyzes the residuals for independence over time points (autocorrelation). The Durbin-Watson statistic will always have a value ranging between 0 and 4. A value of 2.0 indicates there is no autocorrelation detected in the sample. Values from 0 to less than 2 point to positive autocorrelation and values from 2 to 4 means negative autocorrelation. The closer to 0 the statistic, the more evidence for positive serial correlation. The closer to 4, the more evidence for negative serial correlation.\n","\n","Durbin-Watson test analyzes the following hypotheses,\n","\n","Null hypothesis (H\u003csub\u003e0\u003c/sub\u003e): Residuals from the regression are not autocorrelated (autocorrelation coefficient, ρ = 0)\n","Alternative hypothesis (H\u003csub\u003ea\u003c/sub\u003e): Residuals from the regression are autocorrelated (autocorrelation coefficient, ρ \u003e 0)\n","\n","A rule of thumb is that DW test statistic values in the range of 1.5 to 2.5 are relatively normal. Values outside this range could, however, be a cause for concern. The Durbin–Watson statistic, while displayed by many regression analysis programs, is not applicable in certain situations. \n","\n","\u003ccenter\u003e\u003ch1\u003e$DW = \\frac{\\sum_{t=2}^{T} ((e_{t} \\; - \\; e_{t-1})^{2}) }{ \\sum_{t=1}^{T} e^{2}_{t} }$\u003c/h1\u003e\u003c/center\u003e\n"]},{"cell_type":"markdown","id":"b95a5694-e55f-435a-a758-5d75c4d9c08e","metadata":{},"outputs":[],"source":["We will use \u003ccode\u003estatsmodels.stats.stattools.durbin_watson\u003c/code\u003e for Durbin-Watson Test and \u003ccode\u003esm.OLS\u003c/code\u003e to get residuals from `statsmodels` library\n"]},{"cell_type":"markdown","id":"e7343856-d68d-4281-8fee-ce6375e09390","metadata":{},"outputs":[],"source":["Let's define function that will return durbin-watson score\n"]},{"cell_type":"code","id":"ec31aa61-99c3-4d51-b0a5-da218546599c","metadata":{},"outputs":[],"source":["def dw_test(df: pd.DataFrame, ind_col: str, dep_col) -\u003e float:\n    \"\"\"\n    Does Durbin-Watson test and return result as float\n    \n    Parameters\n    ----------\n    df: pd.DataFrame\n        Pandas dataframe that needs to contain columns `ind_col` and `dep_col`\n    ind_col: str\n        Name of independant currency\n    dep_col: str\n        Name of dependant currency\n    \n    Returns\n    -------\n    score: float\n        Durbin-Watson score which has range of [0, 4]\n    \"\"\"\n    \n    # We want to check on autocorrelation so we suppose that {ind_col} is depandant on {dep_col}\n    X = df[ind_col] # independent\n    y = df[dep_col] # dependent\n    # to get intercept\n    X = sm.add_constant(X)\n    # fit the regression model\n    reg = sm.OLS(y, X).fit()\n    score = dwtest(resids=np.array(reg.resid))\n    return score"]},{"cell_type":"markdown","id":"c4a647b5-ff48-4cc9-8b36-e28a1807cae5","metadata":{},"outputs":[],"source":["Let's try this function on **\"Avg_price\"**, **\"AD\"** columns\n"]},{"cell_type":"code","id":"048e2995-ae58-4c64-9c9b-ad73e8f50ec1","metadata":{},"outputs":[],"source":["# We want to check on autocorrelation so we suppose that \"Avg_price\" is depandant on \"AD\"\ndw_test(df, \"AD\", \"Avg_price\")"]},{"cell_type":"markdown","id":"fe9183d0-ddf1-4427-9b97-c58ea78b60c3","metadata":{},"outputs":[],"source":["## **Conclusion:**\n","\n","Because the score is very close to 0 we conclude that there is low positive autocorrelation\n"]},{"cell_type":"markdown","id":"d0d7d783-1be4-4196-b4c2-24338c938dca","metadata":{},"outputs":[],"source":["Let's calculate Durbin-Watson for every possible pair\n"]},{"cell_type":"code","id":"f89dec62-224f-46df-b0b8-f55c7f8c9bb9","metadata":{},"outputs":[],"source":["dw_elements = [\"ATR\", \"OBV\", \"RSI\", \"AD\", \"Avg_price\"]\n\ncols = [f\"{el}_dep\" for el in dw_elements]\nidxs = [f\"{el}_ind\" for el in dw_elements]\n\ndw_df = pd.DataFrame(columns=cols, index=idxs)\n\nfor (curr1, curr2) in itertools.permutations(dw_elements, 2):\n    dw = dw_test(df, curr1, curr2)\n    dw_df.loc[f\"{curr2}_ind\", f\"{curr1}_dep\"] = dw\n    \nnp.fill_diagonal(dw_df.values, \"—\")\n    \ndw_df"]},{"cell_type":"markdown","id":"e169996a-94b3-4ddf-80ab-f96b5866c516","metadata":{},"outputs":[],"source":["## **Conclusion:**\n","\n","Because the scores are very close to 0 we conclude that every pair has positive autocorrelation. However the values are very low that's why autocorrelation is also low.\n"]},{"cell_type":"markdown","id":"832d0500-84d4-496e-9d31-aa1b7a64a095","metadata":{},"outputs":[],"source":["\u003cdiv class=\"alert alert-danger alertdanger\" style=\"margin-top: 20px\"\u003e\n","\u003ch1\u003e\u003cstrong\u003eQuestion 6:\u003c/strong\u003e\u003c/h1\u003e\n","\n","**Get DW score using `dw_test` function on columns \"ATR\" (ind_col), \"Avg_price\" (dep_col)**\n","\u003c/div\u003e\n"]},{"cell_type":"code","id":"b45828a1-a53c-41f2-86bc-295252d51a3a","metadata":{},"outputs":[],"source":["# Write your code below and press Shift+Enter to execute \n"]},{"cell_type":"markdown","id":"bf3fdf85-346f-449e-be40-9f5b1e03ed3c","metadata":{},"outputs":[],"source":["\u003cdetails\u003e\u003csummary\u003eClick here for the solution\u003c/summary\u003e\n","\n","```python\n","dw_test(df, \"ATR\", \"Avg_price\")\n","\n","```\n","\n","\u003c/details\u003e\n"]},{"cell_type":"markdown","id":"710f7851-648d-494e-bf50-618be9b50fc9","metadata":{},"outputs":[],"source":["# **8. Granger Causality Test**\n"]},{"cell_type":"markdown","id":"bbc05937-f9b5-4e14-a56b-4c4e082b0313","metadata":{},"outputs":[],"source":["Granger Causality test is a statistical test that is used to determine if a given time series and it’s lags is helpful in explaining the value of another series. \n"]},{"cell_type":"markdown","id":"83922c5e-57e8-4315-b19c-a309afa3b2c5","metadata":{},"outputs":[],"source":["The Null hypothesis for grangercausalitytests is that the time series in\n","the second column, $x_2$, does NOT Granger cause the time series in the first\n","column, $x_1$. Grange causality means that past values of $x_2$ have a\n","statistically significant effect on the current value of $x_1$, taking past\n","values of $x_1$ into account as regressors. We reject the null hypothesis\n","that $x_2$ does not Granger cause $x_1$ if the p-values are below a desired size\n","of the test.\n"]},{"cell_type":"markdown","id":"b1568721-ffcb-4a6c-a9e7-3f698b59e962","metadata":{},"outputs":[],"source":["**How to interpret the p-values?**\n","\n","Assuming a significance level of 0.05, if the p-value is lesser than 0.05, then we do NOT reject the null hypothesis that $X$ does NOT granger cause $Y$.\n"]},{"cell_type":"markdown","id":"bacefcaa-9926-48a4-bde5-ccbd9a7e75ac","metadata":{},"outputs":[],"source":["Let's define function that will plot 2 variables into 1 plot\n"]},{"cell_type":"code","id":"3c1005b3-dc07-4b23-b94e-13c7b85aed18","metadata":{},"outputs":[],"source":["def plot_two_variables(df: pd.DataFrame, col1: str, col2: str) -\u003e None:\n    \"\"\"\n    Plots `col1` and `col2` currencies into a single plot with adjusted axes\n    \n    Parameters\n    ----------\n    df: pd.DataFrame\n        Pandas dataframe that needs to contain columns `col1` and `col2`\n    col1: str\n        Name of first currency to plot\n    col2: str\n        Name of second currency to plot\n    \"\"\"\n    \n    df_to_test = df[[col1, col2]]\n    x = df[\"Ts\"]\n    y1 = df_to_test[col1]\n    y2 = df_to_test[col2]\n\n    # Plot Line1 (Left Y Axis)\n    fig, ax1 = plt.subplots(1,1,figsize=(16,9), dpi= 80)\n    ax1.plot(x, y1, color=\"tab:red\")\n\n    # Plot Line2 (Right Y Axis)\n    ax2 = ax1.twinx()  # instantiate a second axes that shares the same x-axis\n    ax2.plot(x, y2, color=\"tab:blue\")\n\n    # Decorations\n    # ax1 (left Y axis)\n    ax1.set_xlabel(\"Time\", fontsize=20)\n    ax1.tick_params(axis=\"x\", rotation=0, labelsize=12)\n    ax1.set_ylabel(col1, color=\"tab:red\", fontsize=20)\n    ax1.tick_params(axis=\"y\", rotation=0, labelcolor=\"tab:red\")\n    ax1.grid(alpha=.4)\n\n    # ax2 (right Y axis)\n    ax2.set_ylabel(col2, color=\"tab:blue\", fontsize=20)\n    ax2.tick_params(axis=\"y\", labelcolor=\"tab:blue\")\n    # ax2.set_xticklabels(x[::60], rotation=90, fontdict={\"fontsize\":10})\n    ax2.set_title(\"Visualizing Leading Indicator Phenomenon\", fontsize=22)\n    plt.show()"]},{"cell_type":"code","id":"2093776d-b564-4aa2-88ef-3b3598308efb","metadata":{},"outputs":[],"source":["plot_two_variables(df, \"Avg_price\", \"OBV\")"]},{"cell_type":"markdown","id":"f4624777-ebf5-489c-a82b-a236c719d71a","metadata":{},"outputs":[],"source":["We will use `grangercausalitytests` for Granger Causality Test from `statsmodels` library\n"]},{"cell_type":"markdown","id":"709f32a8-4e9e-496e-ba1f-2d8ccdaf1335","metadata":{},"outputs":[],"source":["Now let's define custom function which will do Granger Causality Test and return result as `pd.DataFrame`\n"]},{"cell_type":"code","id":"8e3d29b9-4211-4195-ab29-db6da6a6ebf9","metadata":{},"outputs":[],"source":["def grangers_causation_matrix(data: pd.DataFrame, maxlag: int, variables: list, test: str =\"ssr_chi2test\", verbose: bool = False):    \n    \"\"\"\n    Check Granger Causality of all possible combinations of the Time series.\n    The rows are the response variable, columns are predictors. The values in the table \n    are the P-Values. P-Values lesser than the significance level (0.05), implies \n    the Null Hypothesis that the coefficients of the corresponding past values is \n    zero, that is, the X does not cause Y can be rejected.\n\n    Parameters\n    ----------\n    data: pd.DataFrame\n        pandas dataframe containing the time series variables\n    maxlag: int\n        Number of lags\n    variables: \n        list containing names of the time series variables\n    test: str\n        Name of test\n    verbose: bool\n        If verbose = True we print in detail\n    \"\"\"\n    df = pd.DataFrame(np.zeros((len(variables), len(variables))), columns=variables, index=variables)\n    for c in df.columns:\n        for r in df.index:\n            test_result = grangercausalitytests(data[[r, c]], maxlag=maxlag, verbose=False)\n            p_values = [round(test_result[i+1][0][test][1],4) for i in range(maxlag)]\n            if verbose: print(f\"Y = {r}, X = {c}, P Values = {p_values}\")\n            min_p_value = np.min(p_values)\n            df.loc[r, c] = min_p_value\n    df.columns = [var + \"_x\" for var in variables]\n    df.index = [var + \"_y\" for var in variables]\n    return df"]},{"cell_type":"code","id":"0e85fc0b-8dd3-45ff-8ea0-e9251aaa1c7a","metadata":{},"outputs":[],"source":["cols_to_test = [\"Avg_price\", \"OBV\"]\ngrangers_causation_matrix(df[cols_to_test], 1, variables=cols_to_test)"]},{"cell_type":"markdown","id":"f0dcd980-b8ac-4e34-9044-d18181d74ee8","metadata":{},"outputs":[],"source":["## **Conclusion:**\n","\n","We can see that 0.2516 $\u003e$ 0.05 and 0.3861 $\u003e$ 0.05 so we conclude that **\"OBV\"** does not granger-cause **\"Avg_price\"** and **\"Avg_price\"** does not granger-cause **\"OBV\"**\n"]},{"cell_type":"markdown","id":"6d315120-47e9-47f5-a217-127e8b42cd95","metadata":{},"outputs":[],"source":["Let's calculate Granger Causality Test for all possible pairs\n"]},{"cell_type":"code","id":"0983bed0-6172-4dc3-8ce0-8ad9cd1f7cee","metadata":{},"outputs":[],"source":["cols = [f\"{el}_x\" for el in dw_elements]\nidxs = [f\"{el}_y\" for el in dw_elements]\n\ngc_df = pd.DataFrame(columns=cols, index=idxs)\n\nfor (curr1, curr2) in itertools.combinations(dw_elements, 2):\n    df_to_test_2 = df[[curr1, curr2]]\n    res_df = grangers_causation_matrix(df_to_test_2, 1, variables=df_to_test_2.columns)\n    p1 = res_df[f\"{curr1}_x\"][f\"{curr2}_y\"]\n    p2 = res_df[f\"{curr2}_x\"][f\"{curr1}_y\"]\n    gc_df.loc[f\"{curr1}_y\", f\"{curr2}_x\"] = p1\n    gc_df.loc[f\"{curr2}_y\", f\"{curr1}_x\"] = p2\n    \nnp.fill_diagonal(gc_df.values, \"—\")\n    \ngc_df"]},{"cell_type":"markdown","id":"fb03fd55-f424-45ef-9f34-101f6d5900bd","metadata":{},"outputs":[],"source":["## **Conclusion:**\n","\n","**\"ATR\"** granger-causes **\"OBV\"**, **\"RSI\"**, **\"AD\"**. **\"Avg_price\"**. **\"OBV\"** granger-causes **\"ATR\"**, **\"RSI\"**, **\"AD\"** granger-causes **\"ATR\"**, **\"OBV\"**. **\"Avg_price\"** granger-causes \"**RSI\"**\n"]},{"cell_type":"markdown","id":"b0b55edd-e9b2-4906-b998-1fc71d2d0618","metadata":{},"outputs":[],"source":["\u003cdiv class=\"alert alert-danger alertdanger\" style=\"margin-top: 20px\"\u003e\n","\u003ch1\u003e\u003cstrong\u003eQuestion 7 a):\u003c/strong\u003e\u003c/h1\u003e\n","\n","**Plot \"ATR\" and \"Avg_price\" using function `plot_two_variables`**\n","\u003c/div\u003e\n"]},{"cell_type":"code","id":"9a187b05-3334-4e8a-94c1-58bc0d4e3360","metadata":{},"outputs":[],"source":["# Write your code below and press Shift+Enter to execute \n"]},{"cell_type":"markdown","id":"2c6544b7-7b76-434a-adc1-6ec835b74a07","metadata":{},"outputs":[],"source":["\u003cdetails\u003e\u003csummary\u003eClick here for the solution\u003c/summary\u003e\n","\n","```python\n","plot_two_variables(df, \"Avg_price\", \"ATR\")\n","\n","```\n","\n","\u003c/details\u003e\n"]},{"cell_type":"markdown","id":"54ea7135-b6a3-4593-bff2-069e5ea6f2de","metadata":{},"outputs":[],"source":["\u003cdiv class=\"alert alert-danger alertdanger\" style=\"margin-top: 20px\"\u003e\n","\u003ch1\u003e\u003cstrong\u003eQuestion 7 b):\u003c/strong\u003e\u003c/h1\u003e\n","\n","**Run Granger Causality Test on \"Avg_price\" and \"OBV\" columns with `maxlag=1`**\n","\u003c/div\u003e\n"]},{"cell_type":"code","id":"7c0b29e2-1c21-4cb5-93cf-9751011183f1","metadata":{},"outputs":[],"source":["# Write your code below and press Shift+Enter to execute \n"]},{"cell_type":"markdown","id":"b5e5d883-6d8b-4168-90e9-7f4fea905bd4","metadata":{},"outputs":[],"source":["\u003cdetails\u003e\u003csummary\u003eClick here for the solution\u003c/summary\u003e\n","\n","```python\n","cols_to_test = [\"Avg_price\", \"ATR\"]\n","grangers_causation_matrix(df[cols_to_test], 1, variables=cols_to_test)\n","\n","```\n","\n","\u003c/details\u003e\n"]},{"cell_type":"markdown","id":"629ae72b-c2ff-43a5-857e-d291c01611f5","metadata":{},"outputs":[],"source":["Let's save our dataset that will be needed for the next lab\n"]},{"cell_type":"code","id":"3af08f30-7b03-4b12-98a2-2cf6cf9ec6c1","metadata":{},"outputs":[],"source":["df.to_csv(\"MATICBUSD_trades_lab3.csv\", index=False)"]},{"cell_type":"markdown","id":"446c16eb-bff5-448c-8d5d-55ab8b6f92c0","metadata":{},"outputs":[],"source":["# **Conclusion:**\n"]},{"cell_type":"markdown","id":"3eb77c83-fd47-4c5d-86fb-02a3cb4ad69e","metadata":{},"outputs":[],"source":["## **Correlation**\n"]},{"cell_type":"markdown","id":"c717ce1b-2c32-4975-bb71-8beea50fd947","metadata":{},"outputs":[],"source":["We now have a better idea of what our data looks like and which indicators are more related to **MATIC/BUSD**.\n","\n","The most related indicators:\n","\n","* **\"ATR\"**\n","* **\"AD\"**\n"]},{"cell_type":"markdown","id":"9b14b401-6e32-4b5e-a4ff-0fd2f0639f19","metadata":{},"outputs":[],"source":["## **Durbin-Watson Test**\n"]},{"cell_type":"markdown","id":"d5d4326f-56b2-48e6-a680-3ae948ff4e78","metadata":{},"outputs":[],"source":["**\"Avg_price\"** has high serial correlation (when it's dependant on) **\"OBV\"**, **\"AD\"**.\n"]},{"cell_type":"markdown","id":"093f96bb-a7ed-4e27-8887-1988277b0649","metadata":{},"outputs":[],"source":["## **Granger Causality Test**\n"]},{"cell_type":"markdown","id":"c5b510a0-efde-4b22-b772-6474b6a86a80","metadata":{},"outputs":[],"source":["**\"ATR\"** granger-causes **\"OBV\"**, **\"RSI\"**, **\"AD\"**. **\"Avg_price\"**. **\"OBV\"** granger-causes **\"ATR\"**, **\"RSI\"**, **\"AD\"** granger-causes **\"ATR\"**, **\"OBV\"**. **\"Avg_price\"** granger-causes **\"RSI\"**.\n","\n","We can conclude that all 4 indicators will be usefull in predicting **\"Avg_price\"**\n","\n","As we now move into building machine learning models to automate our analysis, feeding the model with variables that meaningfully affect our target variable will improve our model's prediction performance.\n"]},{"cell_type":"markdown","id":"17c3ce31-a41b-487c-bf7c-b971469fca9f","metadata":{},"outputs":[],"source":["# **9. Sources**:\n","\n","\u003cul\u003e\n","    \u003cli\u003e\u003ca href=\"https://en.wikipedia.org/wiki/Correlation?utm_medium=Exinfluencer\u0026utm_source=Exinfluencer\u0026utm_content=000026UJ\u0026utm_term=10006555\u0026utm_id=NA-SkillsNetwork-Channel-SkillsNetworkGuidedProjectsIBMSkillsNetworkGPXX0CJ2EN2387-2023-01-01\"\u003ehttps://en.wikipedia.org/wiki/Correlation\u003c/a\u003e\u003c/li\u003e\n","    \u003cli\u003e\u003ca href=\"https://www.investopedia.com/terms/a/anova.asp?utm_medium=Exinfluencer\u0026utm_source=Exinfluencer\u0026utm_content=000026UJ\u0026utm_term=10006555\u0026utm_id=NA-SkillsNetwork-Channel-SkillsNetworkGuidedProjectsIBMSkillsNetworkGPXX0CJ2EN2387-2023-01-01\"\u003ehttps://www.investopedia.com/terms/a/anova.asp\u003c/a\u003e\u003c/li\u003e\n","    \u003cli\u003e\u003ca href=\"https://www.statsmodels.org/dev/generated/statsmodels.stats.stattools.durbin_watson.html?utm_medium=Exinfluencer\u0026utm_source=Exinfluencer\u0026utm_content=000026UJ\u0026utm_term=10006555\u0026utm_id=NA-SkillsNetwork-Channel-SkillsNetworkGuidedProjectsIBMSkillsNetworkGPXX0CJ2EN2387-2023-01-01\"\u003ehttps://www.statsmodels.org/dev/generated/statsmodels.stats.stattools.durbin_watson.html\u003c/a\u003e\u003c/li\u003e\n","    \u003cli\u003e\u003ca href=\"https://www.investopedia.com/terms/d/durbin-watson-statistic.asp?utm_medium=Exinfluencer\u0026utm_source=Exinfluencer\u0026utm_content=000026UJ\u0026utm_term=10006555\u0026utm_id=NA-SkillsNetwork-Channel-SkillsNetworkGuidedProjectsIBMSkillsNetworkGPXX0CJ2EN2387-2023-01-01\"\u003ehttps://www.investopedia.com/terms/d/durbin-watson-statistic.asp\u003c/a\u003e\u003c/li\u003e\n","    \u003cli\u003e\u003ca href=\"https://en.wikipedia.org/wiki/Granger_causality?utm_medium=Exinfluencer\u0026utm_source=Exinfluencer\u0026utm_content=000026UJ\u0026utm_term=10006555\u0026utm_id=NA-SkillsNetwork-Channel-SkillsNetworkGuidedProjectsIBMSkillsNetworkGPXX0CJ2EN2387-2023-01-01\"\u003ehttps://en.wikipedia.org/wiki/Granger_causality\u003c/a\u003e\u003c/li\u003e\n","    \u003cli\u003e\u003ca href=\"https://upload.wikimedia.org/wikipedia/commons/thumb/9/92/Candlestick_chart_scheme_01-en.svg/1024px-Candlestick_chart_scheme_01-en.svg.png?utm_medium=Exinfluencer\u0026utm_source=Exinfluencer\u0026utm_content=000026UJ\u0026utm_term=10006555\u0026utm_id=NA-SkillsNetwork-Channel-SkillsNetworkGuidedProjectsIBMSkillsNetworkGPXX0CJ2EN2387-2023-01-01\"\u003ehttps://upload.wikimedia.org/wikipedia/commons/thumb/9/92/Candlestick_chart_scheme_01-en.svg/1024px-Candlestick_chart_scheme_01-en.svg.png\u003c/a\u003e\u003c/li\u003e\n","    \u003cli\u003e\u003ca href=\"https://upload.wikimedia.org/wikipedia/commons/thumb/d/d4/Correlation_examples2.svg/1920px-Correlation_examples2.svg.png?utm_medium=Exinfluencer\u0026utm_source=Exinfluencer\u0026utm_content=000026UJ\u0026utm_term=10006555\u0026utm_id=NA-SkillsNetwork-Channel-SkillsNetworkGuidedProjectsIBMSkillsNetworkGPXX0CJ2EN2387-2023-01-01\"\u003ehttps://upload.wikimedia.org/wikipedia/commons/thumb/d/d4/Correlation_examples2.svg/1920px-Correlation_examples2.svg.png\u003c/a\u003e\u003c/li\u003e\n","\u003c/ul\u003e\n"]},{"cell_type":"markdown","id":"01ec4ae6-2327-4f86-b0b0-06a38e136e77","metadata":{},"outputs":[],"source":["# **Thank you for completing this lab!**\n","\n","## Author\n","\n","\u003ca href=\"https://author.skills.network/instructors/borys_melnychuk?utm_medium=Exinfluencer\u0026utm_source=Exinfluencer\u0026utm_content=000026UJ\u0026utm_term=10006555\u0026utm_id=NA-SkillsNetwork-Channel-SkillsNetworkGuidedProjectsIBMSkillsNetworkGPXX0CJ2EN2387-2023-01-01\" \u003eBorys Melnychuk\u003c/a\u003e\n","\n","\u003ca href=\"https://author.skills.network/instructors/yaroslav_vyklyuk_2?utm_medium=Exinfluencer\u0026utm_source=Exinfluencer\u0026utm_content=000026UJ\u0026utm_term=10006555\u0026utm_id=NA-SkillsNetwork-Channel-SkillsNetworkGuidedProjectsIBMSkillsNetworkGPXX0QGDEN2306-2023-01-01\"\u003eProf. Yaroslav Vyklyuk, DrSc, PhD\u003c/a\u003e\n","\n","\u003ca href=\"https://author.skills.network/instructors/mariya_fleychuk?utm_medium=Exinfluencer\u0026utm_source=Exinfluencer\u0026utm_content=000026UJ\u0026utm_term=10006555\u0026utm_id=NA-SkillsNetwork-Channel-SkillsNetworkGuidedProjectsIBMSkillsNetworkGPXX0QGDEN2306-2023-01-01\"\u003eProf. Mariya Fleychuk, DrSc, PhD\u003c/a\u003e\n","\n","\n","\n","## Change Log\n","\n","| Date (YYYY-MM-DD) | Version | Changed By      | Change Description                                         |\n","| ----------------- | ------- | ----------------| ---------------------------------------------------------- |\n","|     2023-03-11    |   1.0   | Borys Melnychuk | Creation of the lab                                        |\n","\n","\u003chr\u003e\n","\n","## \u003ch3 align=\"center\"\u003e © IBM Corporation 2023. All rights reserved. \u003c/h3\u003e\n"]},{"cell_type":"code","id":"48c46f4e-4808-4b19-a011-010edae4cff6","metadata":{},"outputs":[],"source":[""]},{"cell_type":"code","id":"68faf248-002d-4a7c-80ba-516ae940a6d7","metadata":{},"outputs":[],"source":[""]}],"metadata":{"kernelspec":{"display_name":"Python","language":"python","name":"conda-env-python-py"},"language_info":{"name":""}},"nbformat":4,"nbformat_minor":4}